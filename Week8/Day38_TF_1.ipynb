{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_WORDS = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz\n",
      "17465344/17464789 [==============================] - 12s 1us/step\n"
     ]
    }
   ],
   "source": [
    "# imdb.npz -> numpy 파일 압축한 것 \n",
    "(train_data, train_labels), (test_data, test_labels) = keras.datasets.imdb.load_data(num_words=NUM_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot 의 개수만큼 output을 맞춘다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'tensorflow.python.keras.api._v2.keras' from '/Users/jung-kyoyoon/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/api/_v2/keras/__init__.py'>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'numpy.version' from '/Users/jung-kyoyoon/anaconda3/lib/python3.7/site-packages/numpy/version.py'>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.16.2'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.version.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.4'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.16.2'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.4-tf'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensorflow.keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((array([list([1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 22665, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 21631, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 19193, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 10311, 8, 4, 107, 117, 5952, 15, 256, 4, 31050, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 12118, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]),\n",
       "         list([1, 194, 1153, 194, 8255, 78, 228, 5, 6, 1463, 4369, 5012, 134, 26, 4, 715, 8, 118, 1634, 14, 394, 20, 13, 119, 954, 189, 102, 5, 207, 110, 3103, 21, 14, 69, 188, 8, 30, 23, 7, 4, 249, 126, 93, 4, 114, 9, 2300, 1523, 5, 647, 4, 116, 9, 35, 8163, 4, 229, 9, 340, 1322, 4, 118, 9, 4, 130, 4901, 19, 4, 1002, 5, 89, 29, 952, 46, 37, 4, 455, 9, 45, 43, 38, 1543, 1905, 398, 4, 1649, 26, 6853, 5, 163, 11, 3215, 10156, 4, 1153, 9, 194, 775, 7, 8255, 11596, 349, 2637, 148, 605, 15358, 8003, 15, 123, 125, 68, 23141, 6853, 15, 349, 165, 4362, 98, 5, 4, 228, 9, 43, 36893, 1157, 15, 299, 120, 5, 120, 174, 11, 220, 175, 136, 50, 9, 4373, 228, 8255, 5, 25249, 656, 245, 2350, 5, 4, 9837, 131, 152, 491, 18, 46151, 32, 7464, 1212, 14, 9, 6, 371, 78, 22, 625, 64, 1382, 9, 8, 168, 145, 23, 4, 1690, 15, 16, 4, 1355, 5, 28, 6, 52, 154, 462, 33, 89, 78, 285, 16, 145, 95]),\n",
       "         list([1, 14, 47, 8, 30, 31, 7, 4, 249, 108, 7, 4, 5974, 54, 61, 369, 13, 71, 149, 14, 22, 112, 4, 2401, 311, 12, 16, 3711, 33, 75, 43, 1829, 296, 4, 86, 320, 35, 534, 19, 263, 4821, 1301, 4, 1873, 33, 89, 78, 12, 66, 16, 4, 360, 7, 4, 58, 316, 334, 11, 4, 1716, 43, 645, 662, 8, 257, 85, 1200, 42, 1228, 2578, 83, 68, 3912, 15, 36, 165, 1539, 278, 36, 69, 44076, 780, 8, 106, 14, 6905, 1338, 18, 6, 22, 12, 215, 28, 610, 40, 6, 87, 326, 23, 2300, 21, 23, 22, 12, 272, 40, 57, 31, 11, 4, 22, 47, 6, 2307, 51, 9, 170, 23, 595, 116, 595, 1352, 13, 191, 79, 638, 89, 51428, 14, 9, 8, 106, 607, 624, 35, 534, 6, 227, 7, 129, 113]),\n",
       "         ...,\n",
       "         list([1, 11, 6, 230, 245, 6401, 9, 6, 1225, 446, 86527, 45, 2174, 84, 8322, 4007, 21, 4, 912, 84, 14532, 325, 725, 134, 15271, 1715, 84, 5, 36, 28, 57, 1099, 21, 8, 140, 8, 703, 5, 11656, 84, 56, 18, 1644, 14, 9, 31, 7, 4, 9406, 1209, 2295, 26094, 1008, 18, 6, 20, 207, 110, 563, 12, 8, 2901, 17793, 8, 97, 6, 20, 53, 4767, 74, 4, 460, 364, 1273, 29, 270, 11, 960, 108, 45, 40, 29, 2961, 395, 11, 6, 4065, 500, 7, 14492, 89, 364, 70, 29, 140, 4, 64, 4780, 11, 4, 2678, 26, 178, 4, 529, 443, 17793, 5, 27, 710, 117, 74936, 8123, 165, 47, 84, 37, 131, 818, 14, 595, 10, 10, 61, 1242, 1209, 10, 10, 288, 2260, 1702, 34, 2901, 17793, 4, 65, 496, 4, 231, 7, 790, 5, 6, 320, 234, 2766, 234, 1119, 1574, 7, 496, 4, 139, 929, 2901, 17793, 7750, 5, 4241, 18, 4, 8497, 13164, 250, 11, 1818, 7561, 4, 4217, 5408, 747, 1115, 372, 1890, 1006, 541, 9303, 7, 4, 59, 11027, 4, 3586, 22459]),\n",
       "         list([1, 1446, 7079, 69, 72, 3305, 13, 610, 930, 8, 12, 582, 23, 5, 16, 484, 685, 54, 349, 11, 4120, 2959, 45, 58, 1466, 13, 197, 12, 16, 43, 23, 21469, 5, 62, 30, 145, 402, 11, 4131, 51, 575, 32, 61, 369, 71, 66, 770, 12, 1054, 75, 100, 2198, 8, 4, 105, 37, 69, 147, 712, 75, 3543, 44, 257, 390, 5, 69, 263, 514, 105, 50, 286, 1814, 23, 4, 123, 13, 161, 40, 5, 421, 4, 116, 16, 897, 13, 40691, 40, 319, 5872, 112, 6700, 11, 4803, 121, 25, 70, 3468, 4, 719, 3798, 13, 18, 31, 62, 40, 8, 7200, 4, 29455, 7, 14, 123, 5, 942, 25, 8, 721, 12, 145, 5, 202, 12, 160, 580, 202, 12, 6, 52, 58, 11418, 92, 401, 728, 12, 39, 14, 251, 8, 15, 251, 5, 21213, 12, 38, 84, 80, 124, 12, 9, 23]),\n",
       "         list([1, 17, 6, 194, 337, 7, 4, 204, 22, 45, 254, 8, 106, 14, 123, 4, 12815, 270, 14437, 5, 16923, 12255, 732, 2098, 101, 405, 39, 14, 1034, 4, 1310, 9, 115, 50, 305, 12, 47, 4, 168, 5, 235, 7, 38, 111, 699, 102, 7, 4, 4039, 9245, 9, 24, 6, 78, 1099, 17, 2345, 16553, 21, 27, 9685, 6139, 5, 29043, 1603, 92, 1183, 4, 1310, 7, 4, 204, 42, 97, 90, 35, 221, 109, 29, 127, 27, 118, 8, 97, 12, 157, 21, 6789, 85010, 9, 6, 66, 78, 1099, 4, 631, 1191, 5, 2642, 272, 191, 1070, 6, 7585, 8, 2197, 70907, 10755, 544, 5, 383, 1271, 848, 1468, 12183, 497, 16876, 8, 1597, 8778, 19280, 21, 60, 27, 239, 9, 43, 8368, 209, 405, 10, 10, 12, 764, 40, 4, 248, 20, 12, 16, 5, 174, 1791, 72, 7, 51, 6, 1739, 22, 4, 204, 131, 9])],\n",
       "        dtype=object), array([1, 0, 0, ..., 0, 1, 0])),\n",
       " (array([list([1, 591, 202, 14, 31, 6, 717, 10, 10, 18142, 10698, 5, 4, 360, 7, 4, 177, 5760, 394, 354, 4, 123, 9, 1035, 1035, 1035, 10, 10, 13, 92, 124, 89, 488, 7944, 100, 28, 1668, 14, 31, 23, 27, 7479, 29, 220, 468, 8, 124, 14, 286, 170, 8, 157, 46, 5, 27, 239, 16, 179, 15387, 38, 32, 25, 7944, 451, 202, 14, 6, 717]),\n",
       "         list([1, 14, 22, 3443, 6, 176, 7, 5063, 88, 12, 2679, 23, 1310, 5, 109, 943, 4, 114, 9, 55, 606, 5, 111, 7, 4, 139, 193, 273, 23, 4, 172, 270, 11, 7216, 10626, 4, 8463, 2801, 109, 1603, 21, 4, 22, 3861, 8, 6, 1193, 1330, 10, 10, 4, 105, 987, 35, 841, 16873, 19, 861, 1074, 5, 1987, 17975, 45, 55, 221, 15, 670, 5304, 526, 14, 1069, 4, 405, 5, 2438, 7, 27, 85, 108, 131, 4, 5045, 5304, 3884, 405, 9, 3523, 133, 5, 50, 13, 104, 51, 66, 166, 14, 22, 157, 9, 4, 530, 239, 34, 8463, 2801, 45, 407, 31, 7, 41, 3778, 105, 21, 59, 299, 12, 38, 950, 5, 4521, 15, 45, 629, 488, 2733, 127, 6, 52, 292, 17, 4, 6936, 185, 132, 1988, 5304, 1799, 488, 2693, 47, 6, 392, 173, 4, 21686, 4378, 270, 2352, 4, 1500, 7, 4, 65, 55, 73, 11, 346, 14, 20, 9, 6, 976, 2078, 7, 5293, 861, 12746, 5, 4182, 30, 3127, 23651, 56, 4, 841, 5, 990, 692, 8, 4, 1669, 398, 229, 10, 10, 13, 2822, 670, 5304, 14, 9, 31, 7, 27, 111, 108, 15, 2033, 19, 7836, 1429, 875, 551, 14, 22, 9, 1193, 21, 45, 4829, 5, 45, 252, 8, 12508, 6, 565, 921, 3639, 39, 4, 529, 48, 25, 181, 8, 67, 35, 1732, 22, 49, 238, 60, 135, 1162, 14, 9, 290, 4, 58, 10, 10, 472, 45, 55, 878, 8, 169, 11, 374, 5687, 25, 203, 28, 8, 818, 12, 125, 4, 3077]),\n",
       "         list([1, 111, 748, 4368, 1133, 33782, 24563, 4, 87, 1551, 1262, 7, 31, 318, 9459, 7, 4, 498, 5076, 748, 63, 29, 5161, 220, 686, 10941, 5, 17, 12, 575, 220, 2507, 17, 6, 185, 132, 24563, 16, 53, 928, 11, 51278, 74, 4, 438, 21, 27, 10044, 589, 8, 22, 107, 20123, 19550, 997, 1638, 8, 35, 2076, 9019, 11, 22, 231, 54, 29, 1706, 29, 100, 18995, 2425, 34, 12998, 8738, 48078, 5, 19353, 98, 31, 2122, 33, 6, 58, 14, 3808, 1638, 8, 4, 365, 7, 2789, 3761, 356, 346, 4, 27608, 1060, 63, 29, 93, 11, 5421, 11, 15236, 33, 6, 58, 54, 1270, 431, 748, 7, 32, 2580, 16, 11, 94, 19469, 10, 10, 4, 993, 45222, 7, 4, 1766, 2634, 2164, 24563, 8, 847, 8, 1450, 121, 31, 7, 27, 86, 2663, 10760, 16, 6, 465, 993, 2006, 30995, 573, 17, 61862, 42, 4, 17345, 37, 473, 6, 711, 6, 8869, 7, 328, 212, 70, 30, 258, 11, 220, 32, 7, 108, 21, 133, 12, 9, 55, 465, 849, 3711, 53, 33, 2071, 1969, 37, 70, 1144, 4, 5940, 1409, 74, 476, 37, 62, 91, 1329, 169, 4, 1330, 10104, 146, 655, 2212, 5, 258, 12, 184, 10104, 546, 5, 849, 10333, 7, 4, 22, 1436, 18, 631, 1386, 797, 7, 4, 8712, 71, 348, 425, 4320, 1061, 19, 10288, 5, 12141, 11, 661, 8, 339, 17863, 4, 2455, 11434, 7, 4, 1962, 10, 10, 263, 787, 9, 270, 11, 6, 9466, 4, 61862, 48414, 121, 4, 5437, 26, 4434, 19, 68, 1372, 5, 28, 446, 6, 318, 7149, 8, 67, 51, 36, 70, 81, 8, 4392, 2294, 36, 1197, 8, 68411, 25399, 18, 6, 711, 4, 9909, 26, 10296, 1125, 11, 14, 636, 720, 12, 426, 28, 77, 776, 8, 97, 38, 111, 7489, 6175, 168, 1239, 5189, 137, 25399, 18, 27, 173, 9, 2399, 17, 6, 12397, 428, 14657, 232, 11, 4, 8014, 37, 272, 40, 2708, 247, 30, 656, 6, 13182, 54, 25399, 3292, 98, 6, 2840, 40, 558, 37, 6093, 98, 4, 17345, 1197, 15, 14, 9, 57, 4893, 5, 4659, 6, 275, 711, 7937, 25399, 3292, 98, 6, 31036, 10, 10, 6639, 19, 14, 10241, 267, 162, 711, 37, 5900, 752, 98, 4, 17345, 2378, 90, 19, 6, 73284, 7, 36744, 1810, 77553, 4, 4770, 3183, 930, 8, 508, 90, 4, 1317, 8, 4, 48414, 17, 15454, 3965, 1853, 4, 1494, 8, 4468, 189, 4, 31036, 6287, 5774, 4, 4770, 5, 95, 271, 23, 6, 7742, 6063, 21627, 5437, 33, 1526, 6, 425, 3155, 33697, 4535, 1636, 7, 4, 4669, 11966, 469, 4, 4552, 54, 4, 150, 5664, 17345, 280, 53, 68411, 25399, 18, 339, 29, 1978, 27, 7885, 5, 17303, 68, 1830, 19, 6571, 14605, 4, 1515, 7, 263, 65, 2132, 34, 6, 5680, 7489, 43, 159, 29, 9, 4706, 9, 387, 73, 195, 584, 10, 10, 1069, 4, 58, 810, 54, 14, 6078, 117, 22, 16, 93, 5, 1069, 4, 192, 15, 12, 16, 93, 34, 6, 1766, 28228, 33, 4, 5673, 7, 15, 18760, 9252, 3286, 325, 12, 62, 30, 776, 8, 67, 14, 17, 6, 12214, 44, 148, 687, 24563, 203, 42, 203, 24, 28, 69, 32157, 6676, 11, 330, 54, 29, 93, 61862, 21, 845, 14148, 27, 1099, 7, 819, 4, 22, 1407, 17, 6, 14967, 787, 7, 2460, 19569, 61862, 100, 30, 4, 3737, 3617, 3169, 2321, 42, 1898, 11, 4, 3814, 42, 101, 704, 7, 101, 999, 15, 1625, 94, 2926, 180, 5, 9, 9101, 34, 15205, 45, 6, 1429, 22, 60, 6, 1220, 31, 11, 94, 6408, 96, 21, 94, 749, 9, 57, 975]),\n",
       "         ...,\n",
       "         list([1, 13, 1408, 15, 8, 135, 14, 9, 35, 32, 46, 394, 20, 62, 30, 5093, 21, 45, 184, 78, 4, 1492, 910, 769, 2290, 2515, 395, 4257, 5, 1454, 11, 119, 16946, 89, 1036, 4, 116, 218, 78, 21, 407, 100, 30, 128, 262, 15, 7, 185, 2280, 284, 1842, 60664, 37, 315, 4, 226, 20, 272, 2942, 40, 29, 152, 60, 181, 8, 30, 50, 553, 362, 80, 119, 12, 21, 846, 5518]),\n",
       "         list([1, 11, 119, 241, 9, 4, 840, 20, 12, 468, 15, 94, 3684, 562, 791, 39, 4, 86, 107, 8, 97, 14, 31, 33, 4, 2960, 7, 743, 46, 1028, 9, 3531, 5, 4, 768, 47, 8, 79, 90, 145, 164, 162, 50, 6, 501, 119, 7, 9, 4, 78, 232, 15, 16, 224, 11, 4, 333, 20, 4, 985, 200, 5, 28739, 5, 9, 1861, 8, 79, 357, 4, 20, 47, 220, 57, 206, 139, 11, 12, 5, 55, 117, 212, 13, 1276, 92, 124, 51, 45, 1188, 71, 536, 13, 520, 14, 20, 6, 2302, 7, 470]),\n",
       "         list([1, 6, 52, 7465, 430, 22, 9, 220, 2594, 8, 28, 24357, 519, 3227, 6, 769, 15, 47, 6, 3482, 4067, 8, 114, 5, 33, 222, 31, 55, 184, 704, 5586, 18020, 19, 346, 3153, 5, 6, 364, 350, 4, 184, 5586, 9, 133, 1810, 11, 5417, 13226, 21, 4, 7298, 42657, 570, 50, 2005, 2643, 9, 6, 1249, 17, 6, 25194, 27803, 21, 17, 6, 1211, 232, 1138, 2249, 29, 266, 56, 96, 346, 194, 308, 9, 194, 21, 29, 218, 1078, 19, 4, 78, 173, 7, 27, 20067, 5698, 3406, 718, 21264, 9, 6, 6907, 17, 210, 5, 3281, 5677, 47, 77, 395, 14, 172, 173, 18, 2740, 2931, 4517, 82, 127, 27, 173, 11, 6, 392, 217, 21, 50, 9, 57, 65, 12, 14274, 53, 40, 35, 390, 7, 11, 4, 3567, 7, 4, 314, 74, 6, 792, 22, 16261, 19, 714, 727, 5205, 382, 4, 91, 6533, 439, 19, 14, 20, 9, 1441, 5805, 1118, 4, 756, 25, 124, 4, 31, 12, 16, 93, 804, 34, 2005, 2643])],\n",
       "        dtype=object),\n",
       "  array([0, 1, 1, ..., 0, 0, 0])))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = sns.load_dataset('iris')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.4</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4.4</td>\n",
       "      <td>2.9</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4.8</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4.8</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.1</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4.3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5.8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>5.7</td>\n",
       "      <td>4.4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>5.7</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.3</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.3</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.5</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>4.8</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>5.2</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>5.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>6.9</td>\n",
       "      <td>3.2</td>\n",
       "      <td>5.7</td>\n",
       "      <td>2.3</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>5.6</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.9</td>\n",
       "      <td>2.0</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>7.7</td>\n",
       "      <td>2.8</td>\n",
       "      <td>6.7</td>\n",
       "      <td>2.0</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.7</td>\n",
       "      <td>4.9</td>\n",
       "      <td>1.8</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.3</td>\n",
       "      <td>5.7</td>\n",
       "      <td>2.1</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>7.2</td>\n",
       "      <td>3.2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.8</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>6.2</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>6.1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.9</td>\n",
       "      <td>1.8</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>6.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.1</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>7.2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.8</td>\n",
       "      <td>1.6</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>7.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>6.1</td>\n",
       "      <td>1.9</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>7.9</td>\n",
       "      <td>3.8</td>\n",
       "      <td>6.4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>6.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.2</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>6.1</td>\n",
       "      <td>2.6</td>\n",
       "      <td>5.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>7.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.1</td>\n",
       "      <td>2.3</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>6.3</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.4</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>6.4</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.5</td>\n",
       "      <td>1.8</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>6.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.1</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.4</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>6.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.1</td>\n",
       "      <td>2.3</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>5.8</td>\n",
       "      <td>2.7</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.9</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>6.8</td>\n",
       "      <td>3.2</td>\n",
       "      <td>5.9</td>\n",
       "      <td>2.3</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.3</td>\n",
       "      <td>5.7</td>\n",
       "      <td>2.5</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal_length  sepal_width  petal_length  petal_width    species\n",
       "0             5.1          3.5           1.4          0.2     setosa\n",
       "1             4.9          3.0           1.4          0.2     setosa\n",
       "2             4.7          3.2           1.3          0.2     setosa\n",
       "3             4.6          3.1           1.5          0.2     setosa\n",
       "4             5.0          3.6           1.4          0.2     setosa\n",
       "5             5.4          3.9           1.7          0.4     setosa\n",
       "6             4.6          3.4           1.4          0.3     setosa\n",
       "7             5.0          3.4           1.5          0.2     setosa\n",
       "8             4.4          2.9           1.4          0.2     setosa\n",
       "9             4.9          3.1           1.5          0.1     setosa\n",
       "10            5.4          3.7           1.5          0.2     setosa\n",
       "11            4.8          3.4           1.6          0.2     setosa\n",
       "12            4.8          3.0           1.4          0.1     setosa\n",
       "13            4.3          3.0           1.1          0.1     setosa\n",
       "14            5.8          4.0           1.2          0.2     setosa\n",
       "15            5.7          4.4           1.5          0.4     setosa\n",
       "16            5.4          3.9           1.3          0.4     setosa\n",
       "17            5.1          3.5           1.4          0.3     setosa\n",
       "18            5.7          3.8           1.7          0.3     setosa\n",
       "19            5.1          3.8           1.5          0.3     setosa\n",
       "20            5.4          3.4           1.7          0.2     setosa\n",
       "21            5.1          3.7           1.5          0.4     setosa\n",
       "22            4.6          3.6           1.0          0.2     setosa\n",
       "23            5.1          3.3           1.7          0.5     setosa\n",
       "24            4.8          3.4           1.9          0.2     setosa\n",
       "25            5.0          3.0           1.6          0.2     setosa\n",
       "26            5.0          3.4           1.6          0.4     setosa\n",
       "27            5.2          3.5           1.5          0.2     setosa\n",
       "28            5.2          3.4           1.4          0.2     setosa\n",
       "29            4.7          3.2           1.6          0.2     setosa\n",
       "..            ...          ...           ...          ...        ...\n",
       "120           6.9          3.2           5.7          2.3  virginica\n",
       "121           5.6          2.8           4.9          2.0  virginica\n",
       "122           7.7          2.8           6.7          2.0  virginica\n",
       "123           6.3          2.7           4.9          1.8  virginica\n",
       "124           6.7          3.3           5.7          2.1  virginica\n",
       "125           7.2          3.2           6.0          1.8  virginica\n",
       "126           6.2          2.8           4.8          1.8  virginica\n",
       "127           6.1          3.0           4.9          1.8  virginica\n",
       "128           6.4          2.8           5.6          2.1  virginica\n",
       "129           7.2          3.0           5.8          1.6  virginica\n",
       "130           7.4          2.8           6.1          1.9  virginica\n",
       "131           7.9          3.8           6.4          2.0  virginica\n",
       "132           6.4          2.8           5.6          2.2  virginica\n",
       "133           6.3          2.8           5.1          1.5  virginica\n",
       "134           6.1          2.6           5.6          1.4  virginica\n",
       "135           7.7          3.0           6.1          2.3  virginica\n",
       "136           6.3          3.4           5.6          2.4  virginica\n",
       "137           6.4          3.1           5.5          1.8  virginica\n",
       "138           6.0          3.0           4.8          1.8  virginica\n",
       "139           6.9          3.1           5.4          2.1  virginica\n",
       "140           6.7          3.1           5.6          2.4  virginica\n",
       "141           6.9          3.1           5.1          2.3  virginica\n",
       "142           5.8          2.7           5.1          1.9  virginica\n",
       "143           6.8          3.2           5.9          2.3  virginica\n",
       "144           6.7          3.3           5.7          2.5  virginica\n",
       "145           6.7          3.0           5.2          2.3  virginica\n",
       "146           6.3          2.5           5.0          1.9  virginica\n",
       "147           6.5          3.0           5.2          2.0  virginica\n",
       "148           6.2          3.4           5.4          2.3  virginica\n",
       "149           5.9          3.0           5.1          1.8  virginica\n",
       "\n",
       "[150 rows x 5 columns]"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "setosa        50\n",
       "versicolor    50\n",
       "virginica     50\n",
       "Name: species, dtype: int64"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.species.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input 4개 / output 3개 => layer는 모름 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation, Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unit 16개 \n",
    "layer1 = Dense(16, input_shape=(4,), activation='relu') # input layer\n",
    "layer2 = Dense(16, activation='relu')\n",
    "output_layer = Dense(3, activation='softmax') # 여러 개가 동시에 있을 때는 softmax (각 노드별로 나올 확률값 알려줌)\n",
    "baseline_model.add(layer1)\n",
    "baseline_model.add(layer2)\n",
    "baseline_model.add(output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_pd = pd.concat([iris,pd.get_dummies(iris.species)],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "      <th>setosa</th>\n",
       "      <th>versicolor</th>\n",
       "      <th>virginica</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.4</td>\n",
       "      <td>setosa</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>setosa</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4.4</td>\n",
       "      <td>2.9</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>setosa</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4.8</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4.8</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.1</td>\n",
       "      <td>setosa</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4.3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>setosa</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5.8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>5.7</td>\n",
       "      <td>4.4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>setosa</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>setosa</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>setosa</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>5.7</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.3</td>\n",
       "      <td>setosa</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>setosa</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>setosa</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.3</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.5</td>\n",
       "      <td>setosa</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>4.8</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>setosa</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>5.2</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>5.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>6.9</td>\n",
       "      <td>3.2</td>\n",
       "      <td>5.7</td>\n",
       "      <td>2.3</td>\n",
       "      <td>virginica</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>5.6</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.9</td>\n",
       "      <td>2.0</td>\n",
       "      <td>virginica</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>7.7</td>\n",
       "      <td>2.8</td>\n",
       "      <td>6.7</td>\n",
       "      <td>2.0</td>\n",
       "      <td>virginica</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.7</td>\n",
       "      <td>4.9</td>\n",
       "      <td>1.8</td>\n",
       "      <td>virginica</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.3</td>\n",
       "      <td>5.7</td>\n",
       "      <td>2.1</td>\n",
       "      <td>virginica</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>7.2</td>\n",
       "      <td>3.2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.8</td>\n",
       "      <td>virginica</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>6.2</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>virginica</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>6.1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.9</td>\n",
       "      <td>1.8</td>\n",
       "      <td>virginica</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>6.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.1</td>\n",
       "      <td>virginica</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>7.2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.8</td>\n",
       "      <td>1.6</td>\n",
       "      <td>virginica</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>7.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>6.1</td>\n",
       "      <td>1.9</td>\n",
       "      <td>virginica</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>7.9</td>\n",
       "      <td>3.8</td>\n",
       "      <td>6.4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>virginica</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>6.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.2</td>\n",
       "      <td>virginica</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>virginica</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>6.1</td>\n",
       "      <td>2.6</td>\n",
       "      <td>5.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>virginica</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>7.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.1</td>\n",
       "      <td>2.3</td>\n",
       "      <td>virginica</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>6.3</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.4</td>\n",
       "      <td>virginica</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>6.4</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.5</td>\n",
       "      <td>1.8</td>\n",
       "      <td>virginica</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>virginica</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>6.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.1</td>\n",
       "      <td>virginica</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.4</td>\n",
       "      <td>virginica</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>6.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.1</td>\n",
       "      <td>2.3</td>\n",
       "      <td>virginica</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>5.8</td>\n",
       "      <td>2.7</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.9</td>\n",
       "      <td>virginica</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>6.8</td>\n",
       "      <td>3.2</td>\n",
       "      <td>5.9</td>\n",
       "      <td>2.3</td>\n",
       "      <td>virginica</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.3</td>\n",
       "      <td>5.7</td>\n",
       "      <td>2.5</td>\n",
       "      <td>virginica</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>virginica</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>virginica</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>virginica</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>virginica</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>virginica</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal_length  sepal_width  petal_length  petal_width    species  setosa  \\\n",
       "0             5.1          3.5           1.4          0.2     setosa       1   \n",
       "1             4.9          3.0           1.4          0.2     setosa       1   \n",
       "2             4.7          3.2           1.3          0.2     setosa       1   \n",
       "3             4.6          3.1           1.5          0.2     setosa       1   \n",
       "4             5.0          3.6           1.4          0.2     setosa       1   \n",
       "5             5.4          3.9           1.7          0.4     setosa       1   \n",
       "6             4.6          3.4           1.4          0.3     setosa       1   \n",
       "7             5.0          3.4           1.5          0.2     setosa       1   \n",
       "8             4.4          2.9           1.4          0.2     setosa       1   \n",
       "9             4.9          3.1           1.5          0.1     setosa       1   \n",
       "10            5.4          3.7           1.5          0.2     setosa       1   \n",
       "11            4.8          3.4           1.6          0.2     setosa       1   \n",
       "12            4.8          3.0           1.4          0.1     setosa       1   \n",
       "13            4.3          3.0           1.1          0.1     setosa       1   \n",
       "14            5.8          4.0           1.2          0.2     setosa       1   \n",
       "15            5.7          4.4           1.5          0.4     setosa       1   \n",
       "16            5.4          3.9           1.3          0.4     setosa       1   \n",
       "17            5.1          3.5           1.4          0.3     setosa       1   \n",
       "18            5.7          3.8           1.7          0.3     setosa       1   \n",
       "19            5.1          3.8           1.5          0.3     setosa       1   \n",
       "20            5.4          3.4           1.7          0.2     setosa       1   \n",
       "21            5.1          3.7           1.5          0.4     setosa       1   \n",
       "22            4.6          3.6           1.0          0.2     setosa       1   \n",
       "23            5.1          3.3           1.7          0.5     setosa       1   \n",
       "24            4.8          3.4           1.9          0.2     setosa       1   \n",
       "25            5.0          3.0           1.6          0.2     setosa       1   \n",
       "26            5.0          3.4           1.6          0.4     setosa       1   \n",
       "27            5.2          3.5           1.5          0.2     setosa       1   \n",
       "28            5.2          3.4           1.4          0.2     setosa       1   \n",
       "29            4.7          3.2           1.6          0.2     setosa       1   \n",
       "..            ...          ...           ...          ...        ...     ...   \n",
       "120           6.9          3.2           5.7          2.3  virginica       0   \n",
       "121           5.6          2.8           4.9          2.0  virginica       0   \n",
       "122           7.7          2.8           6.7          2.0  virginica       0   \n",
       "123           6.3          2.7           4.9          1.8  virginica       0   \n",
       "124           6.7          3.3           5.7          2.1  virginica       0   \n",
       "125           7.2          3.2           6.0          1.8  virginica       0   \n",
       "126           6.2          2.8           4.8          1.8  virginica       0   \n",
       "127           6.1          3.0           4.9          1.8  virginica       0   \n",
       "128           6.4          2.8           5.6          2.1  virginica       0   \n",
       "129           7.2          3.0           5.8          1.6  virginica       0   \n",
       "130           7.4          2.8           6.1          1.9  virginica       0   \n",
       "131           7.9          3.8           6.4          2.0  virginica       0   \n",
       "132           6.4          2.8           5.6          2.2  virginica       0   \n",
       "133           6.3          2.8           5.1          1.5  virginica       0   \n",
       "134           6.1          2.6           5.6          1.4  virginica       0   \n",
       "135           7.7          3.0           6.1          2.3  virginica       0   \n",
       "136           6.3          3.4           5.6          2.4  virginica       0   \n",
       "137           6.4          3.1           5.5          1.8  virginica       0   \n",
       "138           6.0          3.0           4.8          1.8  virginica       0   \n",
       "139           6.9          3.1           5.4          2.1  virginica       0   \n",
       "140           6.7          3.1           5.6          2.4  virginica       0   \n",
       "141           6.9          3.1           5.1          2.3  virginica       0   \n",
       "142           5.8          2.7           5.1          1.9  virginica       0   \n",
       "143           6.8          3.2           5.9          2.3  virginica       0   \n",
       "144           6.7          3.3           5.7          2.5  virginica       0   \n",
       "145           6.7          3.0           5.2          2.3  virginica       0   \n",
       "146           6.3          2.5           5.0          1.9  virginica       0   \n",
       "147           6.5          3.0           5.2          2.0  virginica       0   \n",
       "148           6.2          3.4           5.4          2.3  virginica       0   \n",
       "149           5.9          3.0           5.1          1.8  virginica       0   \n",
       "\n",
       "     versicolor  virginica  \n",
       "0             0          0  \n",
       "1             0          0  \n",
       "2             0          0  \n",
       "3             0          0  \n",
       "4             0          0  \n",
       "5             0          0  \n",
       "6             0          0  \n",
       "7             0          0  \n",
       "8             0          0  \n",
       "9             0          0  \n",
       "10            0          0  \n",
       "11            0          0  \n",
       "12            0          0  \n",
       "13            0          0  \n",
       "14            0          0  \n",
       "15            0          0  \n",
       "16            0          0  \n",
       "17            0          0  \n",
       "18            0          0  \n",
       "19            0          0  \n",
       "20            0          0  \n",
       "21            0          0  \n",
       "22            0          0  \n",
       "23            0          0  \n",
       "24            0          0  \n",
       "25            0          0  \n",
       "26            0          0  \n",
       "27            0          0  \n",
       "28            0          0  \n",
       "29            0          0  \n",
       "..          ...        ...  \n",
       "120           0          1  \n",
       "121           0          1  \n",
       "122           0          1  \n",
       "123           0          1  \n",
       "124           0          1  \n",
       "125           0          1  \n",
       "126           0          1  \n",
       "127           0          1  \n",
       "128           0          1  \n",
       "129           0          1  \n",
       "130           0          1  \n",
       "131           0          1  \n",
       "132           0          1  \n",
       "133           0          1  \n",
       "134           0          1  \n",
       "135           0          1  \n",
       "136           0          1  \n",
       "137           0          1  \n",
       "138           0          1  \n",
       "139           0          1  \n",
       "140           0          1  \n",
       "141           0          1  \n",
       "142           0          1  \n",
       "143           0          1  \n",
       "144           0          1  \n",
       "145           0          1  \n",
       "146           0          1  \n",
       "147           0          1  \n",
       "148           0          1  \n",
       "149           0          1  \n",
       "\n",
       "[150 rows x 8 columns]"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_pd.drop(columns='species',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>setosa</th>\n",
       "      <th>versicolor</th>\n",
       "      <th>virginica</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4.4</td>\n",
       "      <td>2.9</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4.8</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4.8</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4.3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5.8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>5.7</td>\n",
       "      <td>4.4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>5.7</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.3</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>4.8</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>5.2</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>5.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>6.9</td>\n",
       "      <td>3.2</td>\n",
       "      <td>5.7</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>5.6</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.9</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>7.7</td>\n",
       "      <td>2.8</td>\n",
       "      <td>6.7</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.7</td>\n",
       "      <td>4.9</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.3</td>\n",
       "      <td>5.7</td>\n",
       "      <td>2.1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>7.2</td>\n",
       "      <td>3.2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>6.2</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>6.1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.9</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>6.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>7.2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.8</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>7.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>6.1</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>7.9</td>\n",
       "      <td>3.8</td>\n",
       "      <td>6.4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>6.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>6.1</td>\n",
       "      <td>2.6</td>\n",
       "      <td>5.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>7.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.1</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>6.3</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>6.4</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.5</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>6.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>6.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.1</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>5.8</td>\n",
       "      <td>2.7</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>6.8</td>\n",
       "      <td>3.2</td>\n",
       "      <td>5.9</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.3</td>\n",
       "      <td>5.7</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal_length  sepal_width  petal_length  petal_width  setosa  versicolor  \\\n",
       "0             5.1          3.5           1.4          0.2       1           0   \n",
       "1             4.9          3.0           1.4          0.2       1           0   \n",
       "2             4.7          3.2           1.3          0.2       1           0   \n",
       "3             4.6          3.1           1.5          0.2       1           0   \n",
       "4             5.0          3.6           1.4          0.2       1           0   \n",
       "5             5.4          3.9           1.7          0.4       1           0   \n",
       "6             4.6          3.4           1.4          0.3       1           0   \n",
       "7             5.0          3.4           1.5          0.2       1           0   \n",
       "8             4.4          2.9           1.4          0.2       1           0   \n",
       "9             4.9          3.1           1.5          0.1       1           0   \n",
       "10            5.4          3.7           1.5          0.2       1           0   \n",
       "11            4.8          3.4           1.6          0.2       1           0   \n",
       "12            4.8          3.0           1.4          0.1       1           0   \n",
       "13            4.3          3.0           1.1          0.1       1           0   \n",
       "14            5.8          4.0           1.2          0.2       1           0   \n",
       "15            5.7          4.4           1.5          0.4       1           0   \n",
       "16            5.4          3.9           1.3          0.4       1           0   \n",
       "17            5.1          3.5           1.4          0.3       1           0   \n",
       "18            5.7          3.8           1.7          0.3       1           0   \n",
       "19            5.1          3.8           1.5          0.3       1           0   \n",
       "20            5.4          3.4           1.7          0.2       1           0   \n",
       "21            5.1          3.7           1.5          0.4       1           0   \n",
       "22            4.6          3.6           1.0          0.2       1           0   \n",
       "23            5.1          3.3           1.7          0.5       1           0   \n",
       "24            4.8          3.4           1.9          0.2       1           0   \n",
       "25            5.0          3.0           1.6          0.2       1           0   \n",
       "26            5.0          3.4           1.6          0.4       1           0   \n",
       "27            5.2          3.5           1.5          0.2       1           0   \n",
       "28            5.2          3.4           1.4          0.2       1           0   \n",
       "29            4.7          3.2           1.6          0.2       1           0   \n",
       "..            ...          ...           ...          ...     ...         ...   \n",
       "120           6.9          3.2           5.7          2.3       0           0   \n",
       "121           5.6          2.8           4.9          2.0       0           0   \n",
       "122           7.7          2.8           6.7          2.0       0           0   \n",
       "123           6.3          2.7           4.9          1.8       0           0   \n",
       "124           6.7          3.3           5.7          2.1       0           0   \n",
       "125           7.2          3.2           6.0          1.8       0           0   \n",
       "126           6.2          2.8           4.8          1.8       0           0   \n",
       "127           6.1          3.0           4.9          1.8       0           0   \n",
       "128           6.4          2.8           5.6          2.1       0           0   \n",
       "129           7.2          3.0           5.8          1.6       0           0   \n",
       "130           7.4          2.8           6.1          1.9       0           0   \n",
       "131           7.9          3.8           6.4          2.0       0           0   \n",
       "132           6.4          2.8           5.6          2.2       0           0   \n",
       "133           6.3          2.8           5.1          1.5       0           0   \n",
       "134           6.1          2.6           5.6          1.4       0           0   \n",
       "135           7.7          3.0           6.1          2.3       0           0   \n",
       "136           6.3          3.4           5.6          2.4       0           0   \n",
       "137           6.4          3.1           5.5          1.8       0           0   \n",
       "138           6.0          3.0           4.8          1.8       0           0   \n",
       "139           6.9          3.1           5.4          2.1       0           0   \n",
       "140           6.7          3.1           5.6          2.4       0           0   \n",
       "141           6.9          3.1           5.1          2.3       0           0   \n",
       "142           5.8          2.7           5.1          1.9       0           0   \n",
       "143           6.8          3.2           5.9          2.3       0           0   \n",
       "144           6.7          3.3           5.7          2.5       0           0   \n",
       "145           6.7          3.0           5.2          2.3       0           0   \n",
       "146           6.3          2.5           5.0          1.9       0           0   \n",
       "147           6.5          3.0           5.2          2.0       0           0   \n",
       "148           6.2          3.4           5.4          2.3       0           0   \n",
       "149           5.9          3.0           5.1          1.8       0           0   \n",
       "\n",
       "     virginica  \n",
       "0            0  \n",
       "1            0  \n",
       "2            0  \n",
       "3            0  \n",
       "4            0  \n",
       "5            0  \n",
       "6            0  \n",
       "7            0  \n",
       "8            0  \n",
       "9            0  \n",
       "10           0  \n",
       "11           0  \n",
       "12           0  \n",
       "13           0  \n",
       "14           0  \n",
       "15           0  \n",
       "16           0  \n",
       "17           0  \n",
       "18           0  \n",
       "19           0  \n",
       "20           0  \n",
       "21           0  \n",
       "22           0  \n",
       "23           0  \n",
       "24           0  \n",
       "25           0  \n",
       "26           0  \n",
       "27           0  \n",
       "28           0  \n",
       "29           0  \n",
       "..         ...  \n",
       "120          1  \n",
       "121          1  \n",
       "122          1  \n",
       "123          1  \n",
       "124          1  \n",
       "125          1  \n",
       "126          1  \n",
       "127          1  \n",
       "128          1  \n",
       "129          1  \n",
       "130          1  \n",
       "131          1  \n",
       "132          1  \n",
       "133          1  \n",
       "134          1  \n",
       "135          1  \n",
       "136          1  \n",
       "137          1  \n",
       "138          1  \n",
       "139          1  \n",
       "140          1  \n",
       "141          1  \n",
       "142          1  \n",
       "143          1  \n",
       "144          1  \n",
       "145          1  \n",
       "146          1  \n",
       "147          1  \n",
       "148          1  \n",
       "149          1  \n",
       "\n",
       "[150 rows x 7 columns]"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "150/150 [==============================] - 0s 99us/sample - loss: 0.0614 - accuracy: 0.9867\n",
      "Epoch 2/100\n",
      "150/150 [==============================] - 0s 65us/sample - loss: 0.0603 - accuracy: 0.9867\n",
      "Epoch 3/100\n",
      "150/150 [==============================] - 0s 72us/sample - loss: 0.0602 - accuracy: 0.9867\n",
      "Epoch 4/100\n",
      "150/150 [==============================] - 0s 62us/sample - loss: 0.0601 - accuracy: 0.9867\n",
      "Epoch 5/100\n",
      "150/150 [==============================] - 0s 71us/sample - loss: 0.0624 - accuracy: 0.9800\n",
      "Epoch 6/100\n",
      "150/150 [==============================] - 0s 106us/sample - loss: 0.0605 - accuracy: 0.9800\n",
      "Epoch 7/100\n",
      "150/150 [==============================] - 0s 79us/sample - loss: 0.0611 - accuracy: 0.9800\n",
      "Epoch 8/100\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.0597 - accuracy: 0.9867\n",
      "Epoch 9/100\n",
      "150/150 [==============================] - 0s 90us/sample - loss: 0.0605 - accuracy: 0.9867\n",
      "Epoch 10/100\n",
      "150/150 [==============================] - 0s 60us/sample - loss: 0.0610 - accuracy: 0.9800\n",
      "Epoch 11/100\n",
      "150/150 [==============================] - 0s 86us/sample - loss: 0.0611 - accuracy: 0.9800\n",
      "Epoch 12/100\n",
      "150/150 [==============================] - 0s 71us/sample - loss: 0.0590 - accuracy: 0.9800\n",
      "Epoch 13/100\n",
      "150/150 [==============================] - 0s 67us/sample - loss: 0.0586 - accuracy: 0.9800\n",
      "Epoch 14/100\n",
      "150/150 [==============================] - 0s 86us/sample - loss: 0.0640 - accuracy: 0.9867\n",
      "Epoch 15/100\n",
      "150/150 [==============================] - 0s 62us/sample - loss: 0.0603 - accuracy: 0.9867\n",
      "Epoch 16/100\n",
      "150/150 [==============================] - 0s 77us/sample - loss: 0.0591 - accuracy: 0.9867\n",
      "Epoch 17/100\n",
      "150/150 [==============================] - 0s 82us/sample - loss: 0.0599 - accuracy: 0.9800\n",
      "Epoch 18/100\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.0599 - accuracy: 0.9800\n",
      "Epoch 19/100\n",
      "150/150 [==============================] - 0s 64us/sample - loss: 0.0594 - accuracy: 0.9800\n",
      "Epoch 20/100\n",
      "150/150 [==============================] - 0s 51us/sample - loss: 0.0593 - accuracy: 0.9800\n",
      "Epoch 21/100\n",
      "150/150 [==============================] - 0s 87us/sample - loss: 0.0586 - accuracy: 0.9800\n",
      "Epoch 22/100\n",
      "150/150 [==============================] - 0s 70us/sample - loss: 0.0593 - accuracy: 0.9867\n",
      "Epoch 23/100\n",
      "150/150 [==============================] - 0s 78us/sample - loss: 0.0628 - accuracy: 0.9800\n",
      "Epoch 24/100\n",
      "150/150 [==============================] - 0s 61us/sample - loss: 0.0589 - accuracy: 0.9800\n",
      "Epoch 25/100\n",
      "150/150 [==============================] - 0s 60us/sample - loss: 0.0597 - accuracy: 0.9800\n",
      "Epoch 26/100\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.0600 - accuracy: 0.9867\n",
      "Epoch 27/100\n",
      "150/150 [==============================] - 0s 58us/sample - loss: 0.0596 - accuracy: 0.9867\n",
      "Epoch 28/100\n",
      "150/150 [==============================] - 0s 79us/sample - loss: 0.0590 - accuracy: 0.9800\n",
      "Epoch 29/100\n",
      "150/150 [==============================] - 0s 56us/sample - loss: 0.0598 - accuracy: 0.9800\n",
      "Epoch 30/100\n",
      "150/150 [==============================] - 0s 58us/sample - loss: 0.0586 - accuracy: 0.9800\n",
      "Epoch 31/100\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.0594 - accuracy: 0.9867\n",
      "Epoch 32/100\n",
      "150/150 [==============================] - 0s 77us/sample - loss: 0.0587 - accuracy: 0.9867\n",
      "Epoch 33/100\n",
      "150/150 [==============================] - 0s 53us/sample - loss: 0.0583 - accuracy: 0.9800\n",
      "Epoch 34/100\n",
      "150/150 [==============================] - 0s 67us/sample - loss: 0.0578 - accuracy: 0.9867\n",
      "Epoch 35/100\n",
      "150/150 [==============================] - 0s 57us/sample - loss: 0.0593 - accuracy: 0.9800\n",
      "Epoch 36/100\n",
      "150/150 [==============================] - 0s 53us/sample - loss: 0.0586 - accuracy: 0.9867\n",
      "Epoch 37/100\n",
      "150/150 [==============================] - 0s 51us/sample - loss: 0.0593 - accuracy: 0.9800\n",
      "Epoch 38/100\n",
      "150/150 [==============================] - 0s 65us/sample - loss: 0.0592 - accuracy: 0.9800\n",
      "Epoch 39/100\n",
      "150/150 [==============================] - 0s 57us/sample - loss: 0.0578 - accuracy: 0.9867\n",
      "Epoch 40/100\n",
      "150/150 [==============================] - 0s 51us/sample - loss: 0.0590 - accuracy: 0.9800\n",
      "Epoch 41/100\n",
      "150/150 [==============================] - 0s 51us/sample - loss: 0.0581 - accuracy: 0.9800\n",
      "Epoch 42/100\n",
      "150/150 [==============================] - 0s 49us/sample - loss: 0.0588 - accuracy: 0.9800\n",
      "Epoch 43/100\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.0587 - accuracy: 0.9800\n",
      "Epoch 44/100\n",
      "150/150 [==============================] - 0s 67us/sample - loss: 0.0581 - accuracy: 0.9867\n",
      "Epoch 45/100\n",
      "150/150 [==============================] - 0s 57us/sample - loss: 0.0577 - accuracy: 0.9800\n",
      "Epoch 46/100\n",
      "150/150 [==============================] - 0s 60us/sample - loss: 0.0579 - accuracy: 0.9867\n",
      "Epoch 47/100\n",
      "150/150 [==============================] - 0s 75us/sample - loss: 0.0585 - accuracy: 0.9800\n",
      "Epoch 48/100\n",
      "150/150 [==============================] - 0s 60us/sample - loss: 0.0573 - accuracy: 0.9800\n",
      "Epoch 49/100\n",
      "150/150 [==============================] - 0s 61us/sample - loss: 0.0577 - accuracy: 0.9800\n",
      "Epoch 50/100\n",
      "150/150 [==============================] - 0s 82us/sample - loss: 0.0577 - accuracy: 0.9867\n",
      "Epoch 51/100\n",
      "150/150 [==============================] - 0s 56us/sample - loss: 0.0582 - accuracy: 0.9800\n",
      "Epoch 52/100\n",
      "150/150 [==============================] - 0s 66us/sample - loss: 0.0573 - accuracy: 0.9867\n",
      "Epoch 53/100\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.0571 - accuracy: 0.9800\n",
      "Epoch 54/100\n",
      "150/150 [==============================] - 0s 60us/sample - loss: 0.0578 - accuracy: 0.9867\n",
      "Epoch 55/100\n",
      "150/150 [==============================] - 0s 61us/sample - loss: 0.0596 - accuracy: 0.9800\n",
      "Epoch 56/100\n",
      "150/150 [==============================] - 0s 62us/sample - loss: 0.0567 - accuracy: 0.9867\n",
      "Epoch 57/100\n",
      "150/150 [==============================] - 0s 87us/sample - loss: 0.0587 - accuracy: 0.9867\n",
      "Epoch 58/100\n",
      "150/150 [==============================] - 0s 54us/sample - loss: 0.0581 - accuracy: 0.9867\n",
      "Epoch 59/100\n",
      "150/150 [==============================] - 0s 50us/sample - loss: 0.0571 - accuracy: 0.9800\n",
      "Epoch 60/100\n",
      "150/150 [==============================] - 0s 49us/sample - loss: 0.0572 - accuracy: 0.9800\n",
      "Epoch 61/100\n",
      "150/150 [==============================] - 0s 47us/sample - loss: 0.0581 - accuracy: 0.9800\n",
      "Epoch 62/100\n",
      "150/150 [==============================] - 0s 54us/sample - loss: 0.0570 - accuracy: 0.9867\n",
      "Epoch 63/100\n",
      "150/150 [==============================] - 0s 71us/sample - loss: 0.0566 - accuracy: 0.9800\n",
      "Epoch 64/100\n",
      "150/150 [==============================] - 0s 69us/sample - loss: 0.0576 - accuracy: 0.9867\n",
      "Epoch 65/100\n",
      "150/150 [==============================] - 0s 54us/sample - loss: 0.0576 - accuracy: 0.9800\n",
      "Epoch 66/100\n",
      "150/150 [==============================] - 0s 52us/sample - loss: 0.0576 - accuracy: 0.9867\n",
      "Epoch 67/100\n",
      "150/150 [==============================] - 0s 77us/sample - loss: 0.0568 - accuracy: 0.9800\n",
      "Epoch 68/100\n",
      "150/150 [==============================] - 0s 49us/sample - loss: 0.0576 - accuracy: 0.9800\n",
      "Epoch 69/100\n",
      "150/150 [==============================] - 0s 68us/sample - loss: 0.0577 - accuracy: 0.9800\n",
      "Epoch 70/100\n",
      "150/150 [==============================] - 0s 66us/sample - loss: 0.0594 - accuracy: 0.9800\n",
      "Epoch 71/100\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.0582 - accuracy: 0.9867\n",
      "Epoch 72/100\n",
      "150/150 [==============================] - 0s 56us/sample - loss: 0.0567 - accuracy: 0.9867\n",
      "Epoch 73/100\n",
      "150/150 [==============================] - 0s 74us/sample - loss: 0.0571 - accuracy: 0.9867\n",
      "Epoch 74/100\n",
      "150/150 [==============================] - 0s 76us/sample - loss: 0.0587 - accuracy: 0.9800\n",
      "Epoch 75/100\n",
      "150/150 [==============================] - 0s 59us/sample - loss: 0.0569 - accuracy: 0.9800\n",
      "Epoch 76/100\n",
      "150/150 [==============================] - 0s 55us/sample - loss: 0.0573 - accuracy: 0.9800\n",
      "Epoch 77/100\n",
      "150/150 [==============================] - 0s 57us/sample - loss: 0.0567 - accuracy: 0.9800\n",
      "Epoch 78/100\n",
      "150/150 [==============================] - 0s 85us/sample - loss: 0.0567 - accuracy: 0.9867\n",
      "Epoch 79/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150/150 [==============================] - 0s 63us/sample - loss: 0.0575 - accuracy: 0.9867\n",
      "Epoch 80/100\n",
      "150/150 [==============================] - 0s 61us/sample - loss: 0.0566 - accuracy: 0.9867\n",
      "Epoch 81/100\n",
      "150/150 [==============================] - 0s 78us/sample - loss: 0.0566 - accuracy: 0.9800\n",
      "Epoch 82/100\n",
      "150/150 [==============================] - 0s 60us/sample - loss: 0.0563 - accuracy: 0.9867\n",
      "Epoch 83/100\n",
      "150/150 [==============================] - 0s 59us/sample - loss: 0.0572 - accuracy: 0.9867\n",
      "Epoch 84/100\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.0574 - accuracy: 0.9800\n",
      "Epoch 85/100\n",
      "150/150 [==============================] - 0s 65us/sample - loss: 0.0567 - accuracy: 0.9867\n",
      "Epoch 86/100\n",
      "150/150 [==============================] - 0s 69us/sample - loss: 0.0554 - accuracy: 0.9800\n",
      "Epoch 87/100\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.0565 - accuracy: 0.9800\n",
      "Epoch 88/100\n",
      "150/150 [==============================] - 0s 87us/sample - loss: 0.0566 - accuracy: 0.9867\n",
      "Epoch 89/100\n",
      "150/150 [==============================] - 0s 71us/sample - loss: 0.0559 - accuracy: 0.9800\n",
      "Epoch 90/100\n",
      "150/150 [==============================] - 0s 62us/sample - loss: 0.0574 - accuracy: 0.9867\n",
      "Epoch 91/100\n",
      "150/150 [==============================] - 0s 72us/sample - loss: 0.0579 - accuracy: 0.9867\n",
      "Epoch 92/100\n",
      "150/150 [==============================] - 0s 69us/sample - loss: 0.0555 - accuracy: 0.9867\n",
      "Epoch 93/100\n",
      "150/150 [==============================] - 0s 83us/sample - loss: 0.0628 - accuracy: 0.9800\n",
      "Epoch 94/100\n",
      "150/150 [==============================] - 0s 65us/sample - loss: 0.0573 - accuracy: 0.9800\n",
      "Epoch 95/100\n",
      "150/150 [==============================] - 0s 75us/sample - loss: 0.0567 - accuracy: 0.9800\n",
      "Epoch 96/100\n",
      "150/150 [==============================] - 0s 66us/sample - loss: 0.0564 - accuracy: 0.9867\n",
      "Epoch 97/100\n",
      "150/150 [==============================] - 0s 57us/sample - loss: 0.0573 - accuracy: 0.9800\n",
      "Epoch 98/100\n",
      "150/150 [==============================] - 0s 54us/sample - loss: 0.0562 - accuracy: 0.9800\n",
      "Epoch 99/100\n",
      "150/150 [==============================] - 0s 83us/sample - loss: 0.0587 - accuracy: 0.9867\n",
      "Epoch 100/100\n",
      "150/150 [==============================] - 0s 63us/sample - loss: 0.0561 - accuracy: 0.9867\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a64b9eb38>"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_model.fit(iris_pd.iloc[:,:-3],iris_pd.iloc[:,-3:],epochs=100,batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150/150 [==============================] - 0s 363us/sample - loss: 0.1173 - accuracy: 0.9733\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.11729390799999237, 0.97333336]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_model.evaluate(iris_pd.iloc[:,:-3],iris_pd.iloc[:,-3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(iris_pd.iloc[:,:-3],iris_pd.iloc[:,-3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "112/112 [==============================] - 0s 82us/sample - loss: 0.0792 - accuracy: 0.9821\n",
      "Epoch 2/100\n",
      "112/112 [==============================] - 0s 76us/sample - loss: 0.0759 - accuracy: 0.9821\n",
      "Epoch 3/100\n",
      "112/112 [==============================] - 0s 81us/sample - loss: 0.0755 - accuracy: 0.9821\n",
      "Epoch 4/100\n",
      "112/112 [==============================] - 0s 75us/sample - loss: 0.0751 - accuracy: 0.9821\n",
      "Epoch 5/100\n",
      "112/112 [==============================] - 0s 87us/sample - loss: 0.0740 - accuracy: 0.9911\n",
      "Epoch 6/100\n",
      "112/112 [==============================] - 0s 66us/sample - loss: 0.0744 - accuracy: 0.9911\n",
      "Epoch 7/100\n",
      "112/112 [==============================] - 0s 92us/sample - loss: 0.0760 - accuracy: 0.9821\n",
      "Epoch 8/100\n",
      "112/112 [==============================] - 0s 67us/sample - loss: 0.0744 - accuracy: 0.9821\n",
      "Epoch 9/100\n",
      "112/112 [==============================] - 0s 93us/sample - loss: 0.0743 - accuracy: 0.9821\n",
      "Epoch 10/100\n",
      "112/112 [==============================] - 0s 90us/sample - loss: 0.0746 - accuracy: 0.9821\n",
      "Epoch 11/100\n",
      "112/112 [==============================] - 0s 67us/sample - loss: 0.0732 - accuracy: 0.9911\n",
      "Epoch 12/100\n",
      "112/112 [==============================] - 0s 69us/sample - loss: 0.0728 - accuracy: 0.9911\n",
      "Epoch 13/100\n",
      "112/112 [==============================] - 0s 81us/sample - loss: 0.0727 - accuracy: 0.9911\n",
      "Epoch 14/100\n",
      "112/112 [==============================] - 0s 73us/sample - loss: 0.0734 - accuracy: 0.9821\n",
      "Epoch 15/100\n",
      "112/112 [==============================] - 0s 115us/sample - loss: 0.0745 - accuracy: 0.9821\n",
      "Epoch 16/100\n",
      "112/112 [==============================] - 0s 82us/sample - loss: 0.0745 - accuracy: 0.9821\n",
      "Epoch 17/100\n",
      "112/112 [==============================] - 0s 117us/sample - loss: 0.0724 - accuracy: 0.9911\n",
      "Epoch 18/100\n",
      "112/112 [==============================] - 0s 63us/sample - loss: 0.0734 - accuracy: 0.9821\n",
      "Epoch 19/100\n",
      "112/112 [==============================] - 0s 112us/sample - loss: 0.0745 - accuracy: 0.9821\n",
      "Epoch 20/100\n",
      "112/112 [==============================] - 0s 80us/sample - loss: 0.0723 - accuracy: 0.9821\n",
      "Epoch 21/100\n",
      "112/112 [==============================] - 0s 76us/sample - loss: 0.0721 - accuracy: 0.9911\n",
      "Epoch 22/100\n",
      "112/112 [==============================] - 0s 77us/sample - loss: 0.0717 - accuracy: 0.9911\n",
      "Epoch 23/100\n",
      "112/112 [==============================] - 0s 100us/sample - loss: 0.0715 - accuracy: 0.9911\n",
      "Epoch 24/100\n",
      "112/112 [==============================] - 0s 80us/sample - loss: 0.0717 - accuracy: 0.9821\n",
      "Epoch 25/100\n",
      "112/112 [==============================] - 0s 87us/sample - loss: 0.0723 - accuracy: 0.9911\n",
      "Epoch 26/100\n",
      "112/112 [==============================] - 0s 83us/sample - loss: 0.0718 - accuracy: 0.9911\n",
      "Epoch 27/100\n",
      "112/112 [==============================] - 0s 90us/sample - loss: 0.0702 - accuracy: 0.9911\n",
      "Epoch 28/100\n",
      "112/112 [==============================] - 0s 68us/sample - loss: 0.0716 - accuracy: 0.9821\n",
      "Epoch 29/100\n",
      "112/112 [==============================] - 0s 96us/sample - loss: 0.0747 - accuracy: 0.9821\n",
      "Epoch 30/100\n",
      "112/112 [==============================] - 0s 78us/sample - loss: 0.0737 - accuracy: 0.9821\n",
      "Epoch 31/100\n",
      "112/112 [==============================] - 0s 64us/sample - loss: 0.0679 - accuracy: 0.9821\n",
      "Epoch 32/100\n",
      "112/112 [==============================] - 0s 78us/sample - loss: 0.0754 - accuracy: 0.9732\n",
      "Epoch 33/100\n",
      "112/112 [==============================] - 0s 70us/sample - loss: 0.0767 - accuracy: 0.9732\n",
      "Epoch 34/100\n",
      "112/112 [==============================] - 0s 84us/sample - loss: 0.0711 - accuracy: 0.9911\n",
      "Epoch 35/100\n",
      "112/112 [==============================] - 0s 71us/sample - loss: 0.0699 - accuracy: 0.9821\n",
      "Epoch 36/100\n",
      "112/112 [==============================] - 0s 68us/sample - loss: 0.0701 - accuracy: 0.9821\n",
      "Epoch 37/100\n",
      "112/112 [==============================] - 0s 77us/sample - loss: 0.0693 - accuracy: 0.9911\n",
      "Epoch 38/100\n",
      "112/112 [==============================] - 0s 83us/sample - loss: 0.0695 - accuracy: 0.9911\n",
      "Epoch 39/100\n",
      "112/112 [==============================] - 0s 97us/sample - loss: 0.0702 - accuracy: 0.9821\n",
      "Epoch 40/100\n",
      "112/112 [==============================] - 0s 63us/sample - loss: 0.0717 - accuracy: 0.9821\n",
      "Epoch 41/100\n",
      "112/112 [==============================] - 0s 74us/sample - loss: 0.0687 - accuracy: 0.9911\n",
      "Epoch 42/100\n",
      "112/112 [==============================] - 0s 73us/sample - loss: 0.0698 - accuracy: 0.9821\n",
      "Epoch 43/100\n",
      "112/112 [==============================] - 0s 78us/sample - loss: 0.0689 - accuracy: 0.9911\n",
      "Epoch 44/100\n",
      "112/112 [==============================] - 0s 84us/sample - loss: 0.0696 - accuracy: 0.9821\n",
      "Epoch 45/100\n",
      "112/112 [==============================] - 0s 61us/sample - loss: 0.0692 - accuracy: 0.9821\n",
      "Epoch 46/100\n",
      "112/112 [==============================] - 0s 77us/sample - loss: 0.0713 - accuracy: 0.9911\n",
      "Epoch 47/100\n",
      "112/112 [==============================] - 0s 89us/sample - loss: 0.0712 - accuracy: 0.9821\n",
      "Epoch 48/100\n",
      "112/112 [==============================] - 0s 56us/sample - loss: 0.0676 - accuracy: 0.9821\n",
      "Epoch 49/100\n",
      "112/112 [==============================] - 0s 68us/sample - loss: 0.0682 - accuracy: 0.9821\n",
      "Epoch 50/100\n",
      "112/112 [==============================] - 0s 79us/sample - loss: 0.0692 - accuracy: 0.9821\n",
      "Epoch 51/100\n",
      "112/112 [==============================] - 0s 68us/sample - loss: 0.0672 - accuracy: 0.9911\n",
      "Epoch 52/100\n",
      "112/112 [==============================] - 0s 65us/sample - loss: 0.0673 - accuracy: 0.9911\n",
      "Epoch 53/100\n",
      "112/112 [==============================] - 0s 67us/sample - loss: 0.0678 - accuracy: 0.9911\n",
      "Epoch 54/100\n",
      "112/112 [==============================] - 0s 94us/sample - loss: 0.0681 - accuracy: 0.9911\n",
      "Epoch 55/100\n",
      "112/112 [==============================] - 0s 62us/sample - loss: 0.0670 - accuracy: 0.9911\n",
      "Epoch 56/100\n",
      "112/112 [==============================] - 0s 75us/sample - loss: 0.0668 - accuracy: 0.9911\n",
      "Epoch 57/100\n",
      "112/112 [==============================] - 0s 79us/sample - loss: 0.0663 - accuracy: 0.9911\n",
      "Epoch 58/100\n",
      "112/112 [==============================] - 0s 77us/sample - loss: 0.0682 - accuracy: 0.9911\n",
      "Epoch 59/100\n",
      "112/112 [==============================] - 0s 75us/sample - loss: 0.0670 - accuracy: 0.9911\n",
      "Epoch 60/100\n",
      "112/112 [==============================] - 0s 66us/sample - loss: 0.0663 - accuracy: 0.9911\n",
      "Epoch 61/100\n",
      "112/112 [==============================] - 0s 65us/sample - loss: 0.0663 - accuracy: 0.9911\n",
      "Epoch 62/100\n",
      "112/112 [==============================] - 0s 80us/sample - loss: 0.0659 - accuracy: 0.9821\n",
      "Epoch 63/100\n",
      "112/112 [==============================] - 0s 87us/sample - loss: 0.0697 - accuracy: 0.9821\n",
      "Epoch 64/100\n",
      "112/112 [==============================] - 0s 73us/sample - loss: 0.0666 - accuracy: 0.9821\n",
      "Epoch 65/100\n",
      "112/112 [==============================] - 0s 69us/sample - loss: 0.0682 - accuracy: 0.9821\n",
      "Epoch 66/100\n",
      "112/112 [==============================] - 0s 93us/sample - loss: 0.0668 - accuracy: 0.9911\n",
      "Epoch 67/100\n",
      "112/112 [==============================] - 0s 97us/sample - loss: 0.0664 - accuracy: 0.9911\n",
      "Epoch 68/100\n",
      "112/112 [==============================] - 0s 69us/sample - loss: 0.0660 - accuracy: 0.9821\n",
      "Epoch 69/100\n",
      "112/112 [==============================] - 0s 70us/sample - loss: 0.0655 - accuracy: 0.9911\n",
      "Epoch 70/100\n",
      "112/112 [==============================] - 0s 90us/sample - loss: 0.0655 - accuracy: 0.9911\n",
      "Epoch 71/100\n",
      "112/112 [==============================] - 0s 90us/sample - loss: 0.0663 - accuracy: 0.9911\n",
      "Epoch 72/100\n",
      "112/112 [==============================] - 0s 73us/sample - loss: 0.0662 - accuracy: 0.9911\n",
      "Epoch 73/100\n",
      "112/112 [==============================] - 0s 53us/sample - loss: 0.0656 - accuracy: 0.9911\n",
      "Epoch 74/100\n",
      "112/112 [==============================] - 0s 66us/sample - loss: 0.0660 - accuracy: 0.9911\n",
      "Epoch 75/100\n",
      "112/112 [==============================] - 0s 82us/sample - loss: 0.0646 - accuracy: 0.9911\n",
      "Epoch 76/100\n",
      "112/112 [==============================] - 0s 58us/sample - loss: 0.0659 - accuracy: 0.9911\n",
      "Epoch 77/100\n",
      "112/112 [==============================] - 0s 51us/sample - loss: 0.0650 - accuracy: 0.9911\n",
      "Epoch 78/100\n",
      "112/112 [==============================] - 0s 64us/sample - loss: 0.0645 - accuracy: 0.9911\n",
      "Epoch 79/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 0s 88us/sample - loss: 0.0665 - accuracy: 0.9821\n",
      "Epoch 80/100\n",
      "112/112 [==============================] - 0s 56us/sample - loss: 0.0694 - accuracy: 0.9821\n",
      "Epoch 81/100\n",
      "112/112 [==============================] - 0s 75us/sample - loss: 0.0703 - accuracy: 0.9821\n",
      "Epoch 82/100\n",
      "112/112 [==============================] - 0s 64us/sample - loss: 0.0644 - accuracy: 0.9911\n",
      "Epoch 83/100\n",
      "112/112 [==============================] - 0s 67us/sample - loss: 0.0660 - accuracy: 0.9821\n",
      "Epoch 84/100\n",
      "112/112 [==============================] - 0s 85us/sample - loss: 0.0664 - accuracy: 0.9911\n",
      "Epoch 85/100\n",
      "112/112 [==============================] - 0s 73us/sample - loss: 0.0645 - accuracy: 0.9911\n",
      "Epoch 86/100\n",
      "112/112 [==============================] - 0s 97us/sample - loss: 0.0642 - accuracy: 0.9911\n",
      "Epoch 87/100\n",
      "112/112 [==============================] - 0s 88us/sample - loss: 0.0640 - accuracy: 0.9911\n",
      "Epoch 88/100\n",
      "112/112 [==============================] - 0s 67us/sample - loss: 0.0642 - accuracy: 0.9911\n",
      "Epoch 89/100\n",
      "112/112 [==============================] - 0s 79us/sample - loss: 0.0636 - accuracy: 0.9911\n",
      "Epoch 90/100\n",
      "112/112 [==============================] - 0s 62us/sample - loss: 0.0637 - accuracy: 0.9821\n",
      "Epoch 91/100\n",
      "112/112 [==============================] - 0s 84us/sample - loss: 0.0645 - accuracy: 0.9821\n",
      "Epoch 92/100\n",
      "112/112 [==============================] - 0s 71us/sample - loss: 0.0644 - accuracy: 0.9821\n",
      "Epoch 93/100\n",
      "112/112 [==============================] - 0s 69us/sample - loss: 0.0630 - accuracy: 0.9821\n",
      "Epoch 94/100\n",
      "112/112 [==============================] - 0s 94us/sample - loss: 0.0627 - accuracy: 0.9911\n",
      "Epoch 95/100\n",
      "112/112 [==============================] - 0s 70us/sample - loss: 0.0638 - accuracy: 0.9911\n",
      "Epoch 96/100\n",
      "112/112 [==============================] - 0s 69us/sample - loss: 0.0647 - accuracy: 0.9821\n",
      "Epoch 97/100\n",
      "112/112 [==============================] - 0s 76us/sample - loss: 0.0643 - accuracy: 0.9911\n",
      "Epoch 98/100\n",
      "112/112 [==============================] - 0s 58us/sample - loss: 0.0656 - accuracy: 0.9821\n",
      "Epoch 99/100\n",
      "112/112 [==============================] - 0s 58us/sample - loss: 0.0636 - accuracy: 0.9821\n",
      "Epoch 100/100\n",
      "112/112 [==============================] - 0s 52us/sample - loss: 0.0625 - accuracy: 0.9911\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a598a18d0>"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_model.fit(X_train,y_train,epochs=100,batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/38 [==============================] - 0s 120us/sample - loss: 0.0564 - accuracy: 0.9737\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.05636245993848302, 0.9736842]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unit 4개 \n",
    "layer1 = Dense(4, input_shape=(4,), activation='relu')\n",
    "layer2 = Dense(4, activation='relu')\n",
    "output_layer = Dense(3, activation='softmax')\n",
    "model.add(layer1)\n",
    "model.add(layer2)\n",
    "model.add(output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "112/112 [==============================] - 0s 972us/sample - loss: 1.1824 - accuracy: 0.3750\n",
      "Epoch 2/100\n",
      "112/112 [==============================] - 0s 64us/sample - loss: 1.1705 - accuracy: 0.3750\n",
      "Epoch 3/100\n",
      "112/112 [==============================] - 0s 98us/sample - loss: 1.1622 - accuracy: 0.3750\n",
      "Epoch 4/100\n",
      "112/112 [==============================] - 0s 85us/sample - loss: 1.1531 - accuracy: 0.3750\n",
      "Epoch 5/100\n",
      "112/112 [==============================] - 0s 107us/sample - loss: 1.1458 - accuracy: 0.3750\n",
      "Epoch 6/100\n",
      "112/112 [==============================] - 0s 124us/sample - loss: 1.1395 - accuracy: 0.3750\n",
      "Epoch 7/100\n",
      "112/112 [==============================] - 0s 121us/sample - loss: 1.1326 - accuracy: 0.3750\n",
      "Epoch 8/100\n",
      "112/112 [==============================] - 0s 110us/sample - loss: 1.1277 - accuracy: 0.3750\n",
      "Epoch 9/100\n",
      "112/112 [==============================] - 0s 74us/sample - loss: 1.1229 - accuracy: 0.3750\n",
      "Epoch 10/100\n",
      "112/112 [==============================] - 0s 118us/sample - loss: 1.1189 - accuracy: 0.3750\n",
      "Epoch 11/100\n",
      "112/112 [==============================] - 0s 106us/sample - loss: 1.1152 - accuracy: 0.3750\n",
      "Epoch 12/100\n",
      "112/112 [==============================] - 0s 109us/sample - loss: 1.1128 - accuracy: 0.3750\n",
      "Epoch 13/100\n",
      "112/112 [==============================] - 0s 99us/sample - loss: 1.1101 - accuracy: 0.3750\n",
      "Epoch 14/100\n",
      "112/112 [==============================] - 0s 68us/sample - loss: 1.1084 - accuracy: 0.3750\n",
      "Epoch 15/100\n",
      "112/112 [==============================] - 0s 113us/sample - loss: 1.1068 - accuracy: 0.3750\n",
      "Epoch 16/100\n",
      "112/112 [==============================] - 0s 110us/sample - loss: 1.1054 - accuracy: 0.3750\n",
      "Epoch 17/100\n",
      "112/112 [==============================] - 0s 98us/sample - loss: 1.1039 - accuracy: 0.3750\n",
      "Epoch 18/100\n",
      "112/112 [==============================] - 0s 100us/sample - loss: 1.1030 - accuracy: 0.3750\n",
      "Epoch 19/100\n",
      "112/112 [==============================] - 0s 66us/sample - loss: 1.1027 - accuracy: 0.3750\n",
      "Epoch 20/100\n",
      "112/112 [==============================] - 0s 71us/sample - loss: 1.1016 - accuracy: 0.3750\n",
      "Epoch 21/100\n",
      "112/112 [==============================] - 0s 80us/sample - loss: 1.1010 - accuracy: 0.3750\n",
      "Epoch 22/100\n",
      "112/112 [==============================] - 0s 62us/sample - loss: 1.1004 - accuracy: 0.3750\n",
      "Epoch 23/100\n",
      "112/112 [==============================] - 0s 72us/sample - loss: 1.0999 - accuracy: 0.3750\n",
      "Epoch 24/100\n",
      "112/112 [==============================] - 0s 76us/sample - loss: 1.0998 - accuracy: 0.3750\n",
      "Epoch 25/100\n",
      "112/112 [==============================] - 0s 81us/sample - loss: 1.0994 - accuracy: 0.3750\n",
      "Epoch 26/100\n",
      "112/112 [==============================] - 0s 62us/sample - loss: 1.0993 - accuracy: 0.3750\n",
      "Epoch 27/100\n",
      "112/112 [==============================] - 0s 64us/sample - loss: 1.0990 - accuracy: 0.3750\n",
      "Epoch 28/100\n",
      "112/112 [==============================] - 0s 83us/sample - loss: 1.0988 - accuracy: 0.3750\n",
      "Epoch 29/100\n",
      "112/112 [==============================] - 0s 67us/sample - loss: 1.0986 - accuracy: 0.3750\n",
      "Epoch 30/100\n",
      "112/112 [==============================] - 0s 86us/sample - loss: 1.0983 - accuracy: 0.3750\n",
      "Epoch 31/100\n",
      "112/112 [==============================] - 0s 94us/sample - loss: 1.0981 - accuracy: 0.3750\n",
      "Epoch 32/100\n",
      "112/112 [==============================] - 0s 79us/sample - loss: 1.0979 - accuracy: 0.3750\n",
      "Epoch 33/100\n",
      "112/112 [==============================] - 0s 83us/sample - loss: 1.0978 - accuracy: 0.3750\n",
      "Epoch 34/100\n",
      "112/112 [==============================] - 0s 71us/sample - loss: 1.0976 - accuracy: 0.3750\n",
      "Epoch 35/100\n",
      "112/112 [==============================] - 0s 75us/sample - loss: 1.0973 - accuracy: 0.3750\n",
      "Epoch 36/100\n",
      "112/112 [==============================] - 0s 87us/sample - loss: 1.0971 - accuracy: 0.3750\n",
      "Epoch 37/100\n",
      "112/112 [==============================] - 0s 65us/sample - loss: 1.0968 - accuracy: 0.3750\n",
      "Epoch 38/100\n",
      "112/112 [==============================] - 0s 90us/sample - loss: 1.0967 - accuracy: 0.3750\n",
      "Epoch 39/100\n",
      "112/112 [==============================] - 0s 79us/sample - loss: 1.0963 - accuracy: 0.3750\n",
      "Epoch 40/100\n",
      "112/112 [==============================] - 0s 56us/sample - loss: 1.0960 - accuracy: 0.3750\n",
      "Epoch 41/100\n",
      "112/112 [==============================] - 0s 92us/sample - loss: 1.0957 - accuracy: 0.3750\n",
      "Epoch 42/100\n",
      "112/112 [==============================] - 0s 77us/sample - loss: 1.0953 - accuracy: 0.3750\n",
      "Epoch 43/100\n",
      "112/112 [==============================] - 0s 55us/sample - loss: 1.0949 - accuracy: 0.3750\n",
      "Epoch 44/100\n",
      "112/112 [==============================] - 0s 73us/sample - loss: 1.0945 - accuracy: 0.3750\n",
      "Epoch 45/100\n",
      "112/112 [==============================] - 0s 65us/sample - loss: 1.0940 - accuracy: 0.3750\n",
      "Epoch 46/100\n",
      "112/112 [==============================] - 0s 76us/sample - loss: 1.0935 - accuracy: 0.3750\n",
      "Epoch 47/100\n",
      "112/112 [==============================] - 0s 61us/sample - loss: 1.0929 - accuracy: 0.3750\n",
      "Epoch 48/100\n",
      "112/112 [==============================] - 0s 100us/sample - loss: 1.0923 - accuracy: 0.3750\n",
      "Epoch 49/100\n",
      "112/112 [==============================] - 0s 69us/sample - loss: 1.0915 - accuracy: 0.3750\n",
      "Epoch 50/100\n",
      "112/112 [==============================] - 0s 66us/sample - loss: 1.0907 - accuracy: 0.3750\n",
      "Epoch 51/100\n",
      "112/112 [==============================] - 0s 70us/sample - loss: 1.0899 - accuracy: 0.3750\n",
      "Epoch 52/100\n",
      "112/112 [==============================] - 0s 71us/sample - loss: 1.0888 - accuracy: 0.3750\n",
      "Epoch 53/100\n",
      "112/112 [==============================] - 0s 65us/sample - loss: 1.0879 - accuracy: 0.3750\n",
      "Epoch 54/100\n",
      "112/112 [==============================] - 0s 68us/sample - loss: 1.0868 - accuracy: 0.3750\n",
      "Epoch 55/100\n",
      "112/112 [==============================] - 0s 76us/sample - loss: 1.0856 - accuracy: 0.3750\n",
      "Epoch 56/100\n",
      "112/112 [==============================] - 0s 83us/sample - loss: 1.0846 - accuracy: 0.3750\n",
      "Epoch 57/100\n",
      "112/112 [==============================] - 0s 91us/sample - loss: 1.0834 - accuracy: 0.3750\n",
      "Epoch 58/100\n",
      "112/112 [==============================] - 0s 66us/sample - loss: 1.0820 - accuracy: 0.3750\n",
      "Epoch 59/100\n",
      "112/112 [==============================] - 0s 77us/sample - loss: 1.0806 - accuracy: 0.3750\n",
      "Epoch 60/100\n",
      "112/112 [==============================] - 0s 91us/sample - loss: 1.0791 - accuracy: 0.3750\n",
      "Epoch 61/100\n",
      "112/112 [==============================] - 0s 91us/sample - loss: 1.0775 - accuracy: 0.3750\n",
      "Epoch 62/100\n",
      "112/112 [==============================] - 0s 65us/sample - loss: 1.0757 - accuracy: 0.3750\n",
      "Epoch 63/100\n",
      "112/112 [==============================] - 0s 79us/sample - loss: 1.0739 - accuracy: 0.3750\n",
      "Epoch 64/100\n",
      "112/112 [==============================] - 0s 70us/sample - loss: 1.0720 - accuracy: 0.3750\n",
      "Epoch 65/100\n",
      "112/112 [==============================] - 0s 99us/sample - loss: 1.0700 - accuracy: 0.3750\n",
      "Epoch 66/100\n",
      "112/112 [==============================] - 0s 76us/sample - loss: 1.0679 - accuracy: 0.3929\n",
      "Epoch 67/100\n",
      "112/112 [==============================] - 0s 92us/sample - loss: 1.0656 - accuracy: 0.3750\n",
      "Epoch 68/100\n",
      "112/112 [==============================] - 0s 79us/sample - loss: 1.0633 - accuracy: 0.3839\n",
      "Epoch 69/100\n",
      "112/112 [==============================] - 0s 70us/sample - loss: 1.0607 - accuracy: 0.3929\n",
      "Epoch 70/100\n",
      "112/112 [==============================] - 0s 101us/sample - loss: 1.0581 - accuracy: 0.3929\n",
      "Epoch 71/100\n",
      "112/112 [==============================] - 0s 75us/sample - loss: 1.0556 - accuracy: 0.4107\n",
      "Epoch 72/100\n",
      "112/112 [==============================] - 0s 82us/sample - loss: 1.0526 - accuracy: 0.4375\n",
      "Epoch 73/100\n",
      "112/112 [==============================] - 0s 99us/sample - loss: 1.0496 - accuracy: 0.4643\n",
      "Epoch 74/100\n",
      "112/112 [==============================] - 0s 67us/sample - loss: 1.0463 - accuracy: 0.4821\n",
      "Epoch 75/100\n",
      "112/112 [==============================] - 0s 92us/sample - loss: 1.0430 - accuracy: 0.5179\n",
      "Epoch 76/100\n",
      "112/112 [==============================] - 0s 75us/sample - loss: 1.0394 - accuracy: 0.5446\n",
      "Epoch 77/100\n",
      "112/112 [==============================] - 0s 115us/sample - loss: 1.0359 - accuracy: 0.5714\n",
      "Epoch 78/100\n",
      "112/112 [==============================] - 0s 86us/sample - loss: 1.0323 - accuracy: 0.5893\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79/100\n",
      "112/112 [==============================] - 0s 94us/sample - loss: 1.0281 - accuracy: 0.5804\n",
      "Epoch 80/100\n",
      "112/112 [==============================] - 0s 77us/sample - loss: 1.0241 - accuracy: 0.5982\n",
      "Epoch 81/100\n",
      "112/112 [==============================] - 0s 87us/sample - loss: 1.0197 - accuracy: 0.5804\n",
      "Epoch 82/100\n",
      "112/112 [==============================] - 0s 70us/sample - loss: 1.0154 - accuracy: 0.5804\n",
      "Epoch 83/100\n",
      "112/112 [==============================] - 0s 90us/sample - loss: 1.0111 - accuracy: 0.5804\n",
      "Epoch 84/100\n",
      "112/112 [==============================] - 0s 75us/sample - loss: 1.0062 - accuracy: 0.5893\n",
      "Epoch 85/100\n",
      "112/112 [==============================] - 0s 71us/sample - loss: 1.0011 - accuracy: 0.6071\n",
      "Epoch 86/100\n",
      "112/112 [==============================] - 0s 92us/sample - loss: 0.9961 - accuracy: 0.5982\n",
      "Epoch 87/100\n",
      "112/112 [==============================] - 0s 82us/sample - loss: 0.9910 - accuracy: 0.6071\n",
      "Epoch 88/100\n",
      "112/112 [==============================] - 0s 76us/sample - loss: 0.9856 - accuracy: 0.6071\n",
      "Epoch 89/100\n",
      "112/112 [==============================] - 0s 89us/sample - loss: 0.9801 - accuracy: 0.6250\n",
      "Epoch 90/100\n",
      "112/112 [==============================] - 0s 83us/sample - loss: 0.9743 - accuracy: 0.6250\n",
      "Epoch 91/100\n",
      "112/112 [==============================] - 0s 77us/sample - loss: 0.9686 - accuracy: 0.6429\n",
      "Epoch 92/100\n",
      "112/112 [==============================] - 0s 67us/sample - loss: 0.9623 - accuracy: 0.6786\n",
      "Epoch 93/100\n",
      "112/112 [==============================] - 0s 94us/sample - loss: 0.9561 - accuracy: 0.6964\n",
      "Epoch 94/100\n",
      "112/112 [==============================] - 0s 86us/sample - loss: 0.9495 - accuracy: 0.7857\n",
      "Epoch 95/100\n",
      "112/112 [==============================] - 0s 82us/sample - loss: 0.9427 - accuracy: 0.8393\n",
      "Epoch 96/100\n",
      "112/112 [==============================] - 0s 91us/sample - loss: 0.9362 - accuracy: 0.8661\n",
      "Epoch 97/100\n",
      "112/112 [==============================] - 0s 76us/sample - loss: 0.9292 - accuracy: 0.9107\n",
      "Epoch 98/100\n",
      "112/112 [==============================] - 0s 71us/sample - loss: 0.9221 - accuracy: 0.9107\n",
      "Epoch 99/100\n",
      "112/112 [==============================] - 0s 88us/sample - loss: 0.9147 - accuracy: 0.9018\n",
      "Epoch 100/100\n",
      "112/112 [==============================] - 0s 88us/sample - loss: 0.9073 - accuracy: 0.8839\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a59cede10>"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train,y_train,epochs=100,batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/38 [==============================] - 0s 2ms/sample - loss: 0.8982 - accuracy: 0.9737\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.8982194724835848, 0.9736842]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unit 512 \n",
    "model2 = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer1 = Dense(512, input_shape=(4,), activation='relu')\n",
    "layer2 = Dense(512, activation='relu')\n",
    "output_layer = Dense(3, activation='softmax')\n",
    "model2.add(layer1)\n",
    "model2.add(layer2)\n",
    "model2.add(output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "112/112 [==============================] - 0s 1ms/sample - loss: 1.0304 - accuracy: 0.4286\n",
      "Epoch 2/100\n",
      "112/112 [==============================] - 0s 164us/sample - loss: 0.7849 - accuracy: 0.6518\n",
      "Epoch 3/100\n",
      "112/112 [==============================] - 0s 217us/sample - loss: 0.6121 - accuracy: 0.6786\n",
      "Epoch 4/100\n",
      "112/112 [==============================] - 0s 229us/sample - loss: 0.5331 - accuracy: 0.6875\n",
      "Epoch 5/100\n",
      "112/112 [==============================] - 0s 183us/sample - loss: 0.4072 - accuracy: 0.8304\n",
      "Epoch 6/100\n",
      "112/112 [==============================] - 0s 184us/sample - loss: 0.4052 - accuracy: 0.7946\n",
      "Epoch 7/100\n",
      "112/112 [==============================] - 0s 216us/sample - loss: 0.3333 - accuracy: 0.9286\n",
      "Epoch 8/100\n",
      "112/112 [==============================] - 0s 136us/sample - loss: 0.3105 - accuracy: 0.9375\n",
      "Epoch 9/100\n",
      "112/112 [==============================] - 0s 104us/sample - loss: 0.2661 - accuracy: 0.9464\n",
      "Epoch 10/100\n",
      "112/112 [==============================] - 0s 124us/sample - loss: 0.2551 - accuracy: 0.9286\n",
      "Epoch 11/100\n",
      "112/112 [==============================] - 0s 123us/sample - loss: 0.2173 - accuracy: 0.9464\n",
      "Epoch 12/100\n",
      "112/112 [==============================] - 0s 103us/sample - loss: 0.1982 - accuracy: 0.9643\n",
      "Epoch 13/100\n",
      "112/112 [==============================] - 0s 144us/sample - loss: 0.1781 - accuracy: 0.9643\n",
      "Epoch 14/100\n",
      "112/112 [==============================] - 0s 157us/sample - loss: 0.1613 - accuracy: 0.9732\n",
      "Epoch 15/100\n",
      "112/112 [==============================] - 0s 129us/sample - loss: 0.1503 - accuracy: 0.9821\n",
      "Epoch 16/100\n",
      "112/112 [==============================] - 0s 180us/sample - loss: 0.1427 - accuracy: 0.9643\n",
      "Epoch 17/100\n",
      "112/112 [==============================] - 0s 144us/sample - loss: 0.1394 - accuracy: 0.9643\n",
      "Epoch 18/100\n",
      "112/112 [==============================] - 0s 147us/sample - loss: 0.1295 - accuracy: 0.9464\n",
      "Epoch 19/100\n",
      "112/112 [==============================] - 0s 145us/sample - loss: 0.1260 - accuracy: 0.9732\n",
      "Epoch 20/100\n",
      "112/112 [==============================] - 0s 126us/sample - loss: 0.1095 - accuracy: 0.9732\n",
      "Epoch 21/100\n",
      "112/112 [==============================] - 0s 117us/sample - loss: 0.1138 - accuracy: 0.9643\n",
      "Epoch 22/100\n",
      "112/112 [==============================] - 0s 129us/sample - loss: 0.1222 - accuracy: 0.9643\n",
      "Epoch 23/100\n",
      "112/112 [==============================] - 0s 119us/sample - loss: 0.1263 - accuracy: 0.9286\n",
      "Epoch 24/100\n",
      "112/112 [==============================] - 0s 109us/sample - loss: 0.0957 - accuracy: 0.9821\n",
      "Epoch 25/100\n",
      "112/112 [==============================] - 0s 125us/sample - loss: 0.1105 - accuracy: 0.9554\n",
      "Epoch 26/100\n",
      "112/112 [==============================] - 0s 119us/sample - loss: 0.1147 - accuracy: 0.9286\n",
      "Epoch 27/100\n",
      "112/112 [==============================] - 0s 105us/sample - loss: 0.1087 - accuracy: 0.9643\n",
      "Epoch 28/100\n",
      "112/112 [==============================] - 0s 141us/sample - loss: 0.1059 - accuracy: 0.9554\n",
      "Epoch 29/100\n",
      "112/112 [==============================] - 0s 135us/sample - loss: 0.0892 - accuracy: 0.9643\n",
      "Epoch 30/100\n",
      "112/112 [==============================] - 0s 136us/sample - loss: 0.1292 - accuracy: 0.9464\n",
      "Epoch 31/100\n",
      "112/112 [==============================] - 0s 121us/sample - loss: 0.1135 - accuracy: 0.9464\n",
      "Epoch 32/100\n",
      "112/112 [==============================] - 0s 142us/sample - loss: 0.1107 - accuracy: 0.9554\n",
      "Epoch 33/100\n",
      "112/112 [==============================] - 0s 137us/sample - loss: 0.0967 - accuracy: 0.9643\n",
      "Epoch 34/100\n",
      "112/112 [==============================] - 0s 152us/sample - loss: 0.0923 - accuracy: 0.9554\n",
      "Epoch 35/100\n",
      "112/112 [==============================] - 0s 131us/sample - loss: 0.0991 - accuracy: 0.9554\n",
      "Epoch 36/100\n",
      "112/112 [==============================] - 0s 145us/sample - loss: 0.0897 - accuracy: 0.9554\n",
      "Epoch 37/100\n",
      "112/112 [==============================] - 0s 138us/sample - loss: 0.0880 - accuracy: 0.9732\n",
      "Epoch 38/100\n",
      "112/112 [==============================] - 0s 148us/sample - loss: 0.0765 - accuracy: 0.9732\n",
      "Epoch 39/100\n",
      "112/112 [==============================] - 0s 145us/sample - loss: 0.0827 - accuracy: 0.9554\n",
      "Epoch 40/100\n",
      "112/112 [==============================] - 0s 157us/sample - loss: 0.0816 - accuracy: 0.9732\n",
      "Epoch 41/100\n",
      "112/112 [==============================] - 0s 147us/sample - loss: 0.0786 - accuracy: 0.9554\n",
      "Epoch 42/100\n",
      "112/112 [==============================] - 0s 156us/sample - loss: 0.0732 - accuracy: 0.9911\n",
      "Epoch 43/100\n",
      "112/112 [==============================] - 0s 164us/sample - loss: 0.0769 - accuracy: 0.9732\n",
      "Epoch 44/100\n",
      "112/112 [==============================] - 0s 129us/sample - loss: 0.0722 - accuracy: 0.9821\n",
      "Epoch 45/100\n",
      "112/112 [==============================] - 0s 142us/sample - loss: 0.0902 - accuracy: 0.9554\n",
      "Epoch 46/100\n",
      "112/112 [==============================] - 0s 134us/sample - loss: 0.0684 - accuracy: 0.9911\n",
      "Epoch 47/100\n",
      "112/112 [==============================] - 0s 152us/sample - loss: 0.0761 - accuracy: 0.9732\n",
      "Epoch 48/100\n",
      "112/112 [==============================] - 0s 153us/sample - loss: 0.0798 - accuracy: 0.9643\n",
      "Epoch 49/100\n",
      "112/112 [==============================] - 0s 164us/sample - loss: 0.0857 - accuracy: 0.9643\n",
      "Epoch 50/100\n",
      "112/112 [==============================] - 0s 162us/sample - loss: 0.0700 - accuracy: 0.9643\n",
      "Epoch 51/100\n",
      "112/112 [==============================] - 0s 136us/sample - loss: 0.0805 - accuracy: 0.9643\n",
      "Epoch 52/100\n",
      "112/112 [==============================] - 0s 124us/sample - loss: 0.0765 - accuracy: 0.9732\n",
      "Epoch 53/100\n",
      "112/112 [==============================] - 0s 136us/sample - loss: 0.0702 - accuracy: 0.9732\n",
      "Epoch 54/100\n",
      "112/112 [==============================] - 0s 134us/sample - loss: 0.0642 - accuracy: 0.9911\n",
      "Epoch 55/100\n",
      "112/112 [==============================] - 0s 128us/sample - loss: 0.0802 - accuracy: 0.9732\n",
      "Epoch 56/100\n",
      "112/112 [==============================] - 0s 124us/sample - loss: 0.0702 - accuracy: 0.9732\n",
      "Epoch 57/100\n",
      "112/112 [==============================] - 0s 132us/sample - loss: 0.0664 - accuracy: 0.9911\n",
      "Epoch 58/100\n",
      "112/112 [==============================] - 0s 133us/sample - loss: 0.0747 - accuracy: 0.9643\n",
      "Epoch 59/100\n",
      "112/112 [==============================] - 0s 108us/sample - loss: 0.0668 - accuracy: 0.9821\n",
      "Epoch 60/100\n",
      "112/112 [==============================] - 0s 121us/sample - loss: 0.0809 - accuracy: 0.9732\n",
      "Epoch 61/100\n",
      "112/112 [==============================] - 0s 132us/sample - loss: 0.1183 - accuracy: 0.9554\n",
      "Epoch 62/100\n",
      "112/112 [==============================] - 0s 128us/sample - loss: 0.0913 - accuracy: 0.9554\n",
      "Epoch 63/100\n",
      "112/112 [==============================] - 0s 149us/sample - loss: 0.0816 - accuracy: 0.9732\n",
      "Epoch 64/100\n",
      "112/112 [==============================] - 0s 136us/sample - loss: 0.0877 - accuracy: 0.9554\n",
      "Epoch 65/100\n",
      "112/112 [==============================] - 0s 140us/sample - loss: 0.0696 - accuracy: 0.9732\n",
      "Epoch 66/100\n",
      "112/112 [==============================] - 0s 119us/sample - loss: 0.0713 - accuracy: 0.9732\n",
      "Epoch 67/100\n",
      "112/112 [==============================] - 0s 141us/sample - loss: 0.0766 - accuracy: 0.9643\n",
      "Epoch 68/100\n",
      "112/112 [==============================] - 0s 133us/sample - loss: 0.0689 - accuracy: 0.9821\n",
      "Epoch 69/100\n",
      "112/112 [==============================] - 0s 126us/sample - loss: 0.0820 - accuracy: 0.9732\n",
      "Epoch 70/100\n",
      "112/112 [==============================] - 0s 138us/sample - loss: 0.0626 - accuracy: 0.9732\n",
      "Epoch 71/100\n",
      "112/112 [==============================] - 0s 143us/sample - loss: 0.0669 - accuracy: 0.9732\n",
      "Epoch 72/100\n",
      "112/112 [==============================] - 0s 128us/sample - loss: 0.1142 - accuracy: 0.9286\n",
      "Epoch 73/100\n",
      "112/112 [==============================] - 0s 135us/sample - loss: 0.1570 - accuracy: 0.9643\n",
      "Epoch 74/100\n",
      "112/112 [==============================] - 0s 142us/sample - loss: 0.1186 - accuracy: 0.9196\n",
      "Epoch 75/100\n",
      "112/112 [==============================] - 0s 129us/sample - loss: 0.1298 - accuracy: 0.9464\n",
      "Epoch 76/100\n",
      "112/112 [==============================] - 0s 117us/sample - loss: 0.1183 - accuracy: 0.9554\n",
      "Epoch 77/100\n",
      "112/112 [==============================] - 0s 123us/sample - loss: 0.0666 - accuracy: 0.9554\n",
      "Epoch 78/100\n",
      "112/112 [==============================] - 0s 154us/sample - loss: 0.1034 - accuracy: 0.9464\n",
      "Epoch 79/100\n",
      "112/112 [==============================] - 0s 131us/sample - loss: 0.0740 - accuracy: 0.9732\n",
      "Epoch 80/100\n",
      "112/112 [==============================] - 0s 135us/sample - loss: 0.0619 - accuracy: 0.9732\n",
      "Epoch 81/100\n",
      "112/112 [==============================] - 0s 125us/sample - loss: 0.0804 - accuracy: 0.9643\n",
      "Epoch 82/100\n",
      "112/112 [==============================] - 0s 136us/sample - loss: 0.0614 - accuracy: 0.9821\n",
      "Epoch 83/100\n",
      "112/112 [==============================] - 0s 132us/sample - loss: 0.0627 - accuracy: 0.9821\n",
      "Epoch 84/100\n",
      "112/112 [==============================] - 0s 138us/sample - loss: 0.0603 - accuracy: 0.9821\n",
      "Epoch 85/100\n",
      "112/112 [==============================] - 0s 135us/sample - loss: 0.0672 - accuracy: 0.9732\n",
      "Epoch 86/100\n",
      "112/112 [==============================] - 0s 148us/sample - loss: 0.0631 - accuracy: 0.9643\n",
      "Epoch 87/100\n",
      "112/112 [==============================] - 0s 180us/sample - loss: 0.0665 - accuracy: 0.9732\n",
      "Epoch 88/100\n",
      "112/112 [==============================] - 0s 148us/sample - loss: 0.0805 - accuracy: 0.9732\n",
      "Epoch 89/100\n",
      "112/112 [==============================] - 0s 145us/sample - loss: 0.0777 - accuracy: 0.9643\n",
      "Epoch 90/100\n",
      "112/112 [==============================] - 0s 136us/sample - loss: 0.0650 - accuracy: 0.9732\n",
      "Epoch 91/100\n",
      "112/112 [==============================] - 0s 145us/sample - loss: 0.0719 - accuracy: 0.9732\n",
      "Epoch 92/100\n",
      "112/112 [==============================] - 0s 152us/sample - loss: 0.0965 - accuracy: 0.9464\n",
      "Epoch 93/100\n",
      "112/112 [==============================] - 0s 148us/sample - loss: 0.0938 - accuracy: 0.9732\n",
      "Epoch 94/100\n",
      "112/112 [==============================] - 0s 161us/sample - loss: 0.1088 - accuracy: 0.9643\n",
      "Epoch 95/100\n",
      "112/112 [==============================] - 0s 148us/sample - loss: 0.1357 - accuracy: 0.9286\n",
      "Epoch 96/100\n",
      "112/112 [==============================] - 0s 160us/sample - loss: 0.0605 - accuracy: 0.9732\n",
      "Epoch 97/100\n",
      "112/112 [==============================] - 0s 139us/sample - loss: 0.0801 - accuracy: 0.9732\n",
      "Epoch 98/100\n",
      "112/112 [==============================] - 0s 152us/sample - loss: 0.0840 - accuracy: 0.9732\n",
      "Epoch 99/100\n",
      "112/112 [==============================] - 0s 149us/sample - loss: 0.0549 - accuracy: 0.9732\n",
      "Epoch 100/100\n",
      "112/112 [==============================] - 0s 143us/sample - loss: 0.1152 - accuracy: 0.9464\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a5e0e22b0>"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit(X_train,y_train,epochs=100,batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/38 [==============================] - 0s 2ms/sample - loss: 0.0230 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.022978799406892472, 1.0]"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unit을 512개로 하니까 accurcacy = 1.0 => overfitting 문제  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 어떻게 overfitting 을 막는가?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L1 방식 (L1 Regularization) vs L2 방식 (L2 Regularization) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dense 함수 안에 kernel_regularizer=None / bias_regularizer=None => 복잡한 모델로 못 변하게 막는 것 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = Sequential()\n",
    "layer1 = Dense(512, input_shape=(4,), activation='relu', kernel_regularizer='l1')\n",
    "layer2 = Dense(512, activation='relu', kernel_regularizer='l1')\n",
    "output_layer = Dense(3, activation='softmax')\n",
    "model3.add(layer1)\n",
    "model3.add(layer2)\n",
    "model3.add(output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "112/112 [==============================] - 0s 1ms/sample - loss: 99.4101 - accuracy: 0.2500\n",
      "Epoch 2/100\n",
      "112/112 [==============================] - 0s 177us/sample - loss: 89.5837 - accuracy: 0.5804\n",
      "Epoch 3/100\n",
      "112/112 [==============================] - 0s 205us/sample - loss: 80.3345 - accuracy: 0.8661\n",
      "Epoch 4/100\n",
      "112/112 [==============================] - 0s 241us/sample - loss: 71.6539 - accuracy: 0.6875\n",
      "Epoch 5/100\n",
      "112/112 [==============================] - 0s 234us/sample - loss: 63.4883 - accuracy: 0.7768\n",
      "Epoch 6/100\n",
      "112/112 [==============================] - 0s 208us/sample - loss: 55.8481 - accuracy: 0.7321\n",
      "Epoch 7/100\n",
      "112/112 [==============================] - 0s 129us/sample - loss: 48.7080 - accuracy: 0.7321\n",
      "Epoch 8/100\n",
      "112/112 [==============================] - 0s 163us/sample - loss: 42.0802 - accuracy: 0.8750\n",
      "Epoch 9/100\n",
      "112/112 [==============================] - 0s 112us/sample - loss: 35.9580 - accuracy: 0.7679\n",
      "Epoch 10/100\n",
      "112/112 [==============================] - 0s 142us/sample - loss: 30.3542 - accuracy: 0.8304\n",
      "Epoch 11/100\n",
      "112/112 [==============================] - 0s 122us/sample - loss: 25.2590 - accuracy: 0.9821\n",
      "Epoch 12/100\n",
      "112/112 [==============================] - 0s 119us/sample - loss: 20.6900 - accuracy: 0.8036\n",
      "Epoch 13/100\n",
      "112/112 [==============================] - 0s 138us/sample - loss: 16.6340 - accuracy: 0.8571\n",
      "Epoch 14/100\n",
      "112/112 [==============================] - 0s 132us/sample - loss: 13.1326 - accuracy: 0.9643\n",
      "Epoch 15/100\n",
      "112/112 [==============================] - 0s 146us/sample - loss: 10.1320 - accuracy: 0.9375\n",
      "Epoch 16/100\n",
      "112/112 [==============================] - 0s 139us/sample - loss: 7.6844 - accuracy: 0.9018\n",
      "Epoch 17/100\n",
      "112/112 [==============================] - 0s 147us/sample - loss: 5.7710 - accuracy: 0.7679\n",
      "Epoch 18/100\n",
      "112/112 [==============================] - 0s 163us/sample - loss: 4.3741 - accuracy: 0.8839\n",
      "Epoch 19/100\n",
      "112/112 [==============================] - 0s 116us/sample - loss: 3.5181 - accuracy: 0.9375\n",
      "Epoch 20/100\n",
      "112/112 [==============================] - 0s 157us/sample - loss: 3.1370 - accuracy: 0.9554\n",
      "Epoch 21/100\n",
      "112/112 [==============================] - 0s 128us/sample - loss: 2.8251 - accuracy: 0.7321\n",
      "Epoch 22/100\n",
      "112/112 [==============================] - 0s 143us/sample - loss: 2.4233 - accuracy: 0.9732\n",
      "Epoch 23/100\n",
      "112/112 [==============================] - 0s 125us/sample - loss: 2.1202 - accuracy: 0.7768\n",
      "Epoch 24/100\n",
      "112/112 [==============================] - 0s 148us/sample - loss: 1.9525 - accuracy: 0.9732\n",
      "Epoch 25/100\n",
      "112/112 [==============================] - 0s 141us/sample - loss: 1.7970 - accuracy: 0.8661\n",
      "Epoch 26/100\n",
      "112/112 [==============================] - 0s 130us/sample - loss: 1.6809 - accuracy: 0.8036\n",
      "Epoch 27/100\n",
      "112/112 [==============================] - 0s 118us/sample - loss: 1.5913 - accuracy: 0.7768\n",
      "Epoch 28/100\n",
      "112/112 [==============================] - 0s 135us/sample - loss: 1.5112 - accuracy: 0.9375\n",
      "Epoch 29/100\n",
      "112/112 [==============================] - 0s 122us/sample - loss: 1.4422 - accuracy: 0.9018\n",
      "Epoch 30/100\n",
      "112/112 [==============================] - 0s 124us/sample - loss: 1.3822 - accuracy: 0.9196\n",
      "Epoch 31/100\n",
      "112/112 [==============================] - 0s 142us/sample - loss: 1.3357 - accuracy: 0.9464\n",
      "Epoch 32/100\n",
      "112/112 [==============================] - 0s 126us/sample - loss: 1.3025 - accuracy: 0.9375\n",
      "Epoch 33/100\n",
      "112/112 [==============================] - 0s 113us/sample - loss: 1.2701 - accuracy: 0.9375\n",
      "Epoch 34/100\n",
      "112/112 [==============================] - 0s 162us/sample - loss: 1.2396 - accuracy: 0.9554\n",
      "Epoch 35/100\n",
      "112/112 [==============================] - 0s 147us/sample - loss: 1.2118 - accuracy: 0.9911\n",
      "Epoch 36/100\n",
      "112/112 [==============================] - 0s 148us/sample - loss: 1.1880 - accuracy: 0.9821\n",
      "Epoch 37/100\n",
      "112/112 [==============================] - 0s 169us/sample - loss: 1.1643 - accuracy: 0.9732\n",
      "Epoch 38/100\n",
      "112/112 [==============================] - 0s 138us/sample - loss: 1.1504 - accuracy: 0.9107\n",
      "Epoch 39/100\n",
      "112/112 [==============================] - 0s 145us/sample - loss: 1.1308 - accuracy: 0.9464\n",
      "Epoch 40/100\n",
      "112/112 [==============================] - 0s 148us/sample - loss: 1.1242 - accuracy: 0.9554\n",
      "Epoch 41/100\n",
      "112/112 [==============================] - 0s 143us/sample - loss: 1.0971 - accuracy: 0.9732\n",
      "Epoch 42/100\n",
      "112/112 [==============================] - 0s 139us/sample - loss: 1.0774 - accuracy: 0.9732\n",
      "Epoch 43/100\n",
      "112/112 [==============================] - 0s 227us/sample - loss: 1.0594 - accuracy: 0.9821\n",
      "Epoch 44/100\n",
      "112/112 [==============================] - 0s 174us/sample - loss: 1.0446 - accuracy: 0.9732\n",
      "Epoch 45/100\n",
      "112/112 [==============================] - 0s 158us/sample - loss: 1.0360 - accuracy: 0.9821\n",
      "Epoch 46/100\n",
      "112/112 [==============================] - 0s 145us/sample - loss: 1.0374 - accuracy: 0.9375\n",
      "Epoch 47/100\n",
      "112/112 [==============================] - 0s 119us/sample - loss: 1.0216 - accuracy: 0.9643\n",
      "Epoch 48/100\n",
      "112/112 [==============================] - 0s 172us/sample - loss: 1.0018 - accuracy: 0.9732\n",
      "Epoch 49/100\n",
      "112/112 [==============================] - 0s 124us/sample - loss: 0.9883 - accuracy: 0.9732\n",
      "Epoch 50/100\n",
      "112/112 [==============================] - 0s 110us/sample - loss: 0.9771 - accuracy: 0.9732\n",
      "Epoch 51/100\n",
      "112/112 [==============================] - 0s 128us/sample - loss: 0.9670 - accuracy: 0.9643\n",
      "Epoch 52/100\n",
      "112/112 [==============================] - 0s 114us/sample - loss: 0.9521 - accuracy: 0.9911\n",
      "Epoch 53/100\n",
      "112/112 [==============================] - 0s 119us/sample - loss: 0.9443 - accuracy: 0.9732\n",
      "Epoch 54/100\n",
      "112/112 [==============================] - 0s 112us/sample - loss: 0.9354 - accuracy: 0.9732\n",
      "Epoch 55/100\n",
      "112/112 [==============================] - 0s 185us/sample - loss: 0.9229 - accuracy: 0.9732\n",
      "Epoch 56/100\n",
      "112/112 [==============================] - 0s 123us/sample - loss: 0.9130 - accuracy: 0.9732\n",
      "Epoch 57/100\n",
      "112/112 [==============================] - 0s 160us/sample - loss: 0.9146 - accuracy: 0.9554\n",
      "Epoch 58/100\n",
      "112/112 [==============================] - 0s 123us/sample - loss: 0.8968 - accuracy: 0.9732\n",
      "Epoch 59/100\n",
      "112/112 [==============================] - 0s 155us/sample - loss: 0.8919 - accuracy: 0.9643\n",
      "Epoch 60/100\n",
      "112/112 [==============================] - 0s 139us/sample - loss: 0.8807 - accuracy: 0.9821\n",
      "Epoch 61/100\n",
      "112/112 [==============================] - 0s 138us/sample - loss: 0.8742 - accuracy: 0.9732\n",
      "Epoch 62/100\n",
      "112/112 [==============================] - 0s 146us/sample - loss: 0.8678 - accuracy: 0.9643\n",
      "Epoch 63/100\n",
      "112/112 [==============================] - 0s 139us/sample - loss: 0.8586 - accuracy: 0.9732\n",
      "Epoch 64/100\n",
      "112/112 [==============================] - 0s 142us/sample - loss: 0.8592 - accuracy: 0.9554\n",
      "Epoch 65/100\n",
      "112/112 [==============================] - 0s 127us/sample - loss: 0.8459 - accuracy: 0.9821\n",
      "Epoch 66/100\n",
      "112/112 [==============================] - 0s 165us/sample - loss: 0.8472 - accuracy: 0.9821\n",
      "Epoch 67/100\n",
      "112/112 [==============================] - 0s 128us/sample - loss: 0.8304 - accuracy: 0.9821\n",
      "Epoch 68/100\n",
      "112/112 [==============================] - 0s 145us/sample - loss: 0.8245 - accuracy: 0.9821\n",
      "Epoch 69/100\n",
      "112/112 [==============================] - 0s 155us/sample - loss: 0.8276 - accuracy: 0.9643\n",
      "Epoch 70/100\n",
      "112/112 [==============================] - 0s 135us/sample - loss: 0.8173 - accuracy: 0.9732\n",
      "Epoch 71/100\n",
      "112/112 [==============================] - 0s 116us/sample - loss: 0.8058 - accuracy: 0.9821\n",
      "Epoch 72/100\n",
      "112/112 [==============================] - 0s 172us/sample - loss: 0.8125 - accuracy: 0.9554\n",
      "Epoch 73/100\n",
      "112/112 [==============================] - 0s 180us/sample - loss: 0.8046 - accuracy: 0.9732\n",
      "Epoch 74/100\n",
      "112/112 [==============================] - 0s 146us/sample - loss: 0.7968 - accuracy: 0.9821\n",
      "Epoch 75/100\n",
      "112/112 [==============================] - 0s 156us/sample - loss: 0.7942 - accuracy: 0.9821\n",
      "Epoch 76/100\n",
      "112/112 [==============================] - 0s 107us/sample - loss: 0.7823 - accuracy: 0.9821\n",
      "Epoch 77/100\n",
      "112/112 [==============================] - 0s 129us/sample - loss: 0.7772 - accuracy: 0.9911\n",
      "Epoch 78/100\n",
      "112/112 [==============================] - 0s 135us/sample - loss: 0.7736 - accuracy: 0.9821\n",
      "Epoch 79/100\n",
      "112/112 [==============================] - 0s 128us/sample - loss: 0.7670 - accuracy: 0.9821\n",
      "Epoch 80/100\n",
      "112/112 [==============================] - 0s 174us/sample - loss: 0.7739 - accuracy: 0.9464\n",
      "Epoch 81/100\n",
      "112/112 [==============================] - 0s 144us/sample - loss: 0.7623 - accuracy: 0.9821\n",
      "Epoch 82/100\n",
      "112/112 [==============================] - 0s 162us/sample - loss: 0.7604 - accuracy: 0.9732\n",
      "Epoch 83/100\n",
      "112/112 [==============================] - 0s 138us/sample - loss: 0.7545 - accuracy: 0.9732\n",
      "Epoch 84/100\n",
      "112/112 [==============================] - 0s 158us/sample - loss: 0.7541 - accuracy: 0.9732\n",
      "Epoch 85/100\n",
      "112/112 [==============================] - 0s 139us/sample - loss: 0.7440 - accuracy: 0.9821\n",
      "Epoch 86/100\n",
      "112/112 [==============================] - 0s 155us/sample - loss: 0.7418 - accuracy: 0.9821\n",
      "Epoch 87/100\n",
      "112/112 [==============================] - 0s 180us/sample - loss: 0.7369 - accuracy: 0.9821\n",
      "Epoch 88/100\n",
      "112/112 [==============================] - 0s 143us/sample - loss: 0.7365 - accuracy: 0.9821\n",
      "Epoch 89/100\n",
      "112/112 [==============================] - 0s 158us/sample - loss: 0.7317 - accuracy: 0.9821\n",
      "Epoch 90/100\n",
      "112/112 [==============================] - 0s 145us/sample - loss: 0.7314 - accuracy: 0.9732\n",
      "Epoch 91/100\n",
      "112/112 [==============================] - 0s 153us/sample - loss: 0.7254 - accuracy: 0.9732\n",
      "Epoch 92/100\n",
      "112/112 [==============================] - 0s 132us/sample - loss: 0.7240 - accuracy: 0.9821\n",
      "Epoch 93/100\n",
      "112/112 [==============================] - 0s 145us/sample - loss: 0.7218 - accuracy: 0.9821\n",
      "Epoch 94/100\n",
      "112/112 [==============================] - 0s 155us/sample - loss: 0.7208 - accuracy: 0.9821\n",
      "Epoch 95/100\n",
      "112/112 [==============================] - 0s 153us/sample - loss: 0.7121 - accuracy: 0.9821\n",
      "Epoch 96/100\n",
      "112/112 [==============================] - 0s 139us/sample - loss: 0.7107 - accuracy: 0.9821\n",
      "Epoch 97/100\n",
      "112/112 [==============================] - 0s 141us/sample - loss: 0.7086 - accuracy: 0.9821\n",
      "Epoch 98/100\n",
      "112/112 [==============================] - 0s 145us/sample - loss: 0.7040 - accuracy: 0.9911\n",
      "Epoch 99/100\n",
      "112/112 [==============================] - 0s 139us/sample - loss: 0.7000 - accuracy: 0.9911\n",
      "Epoch 100/100\n",
      "112/112 [==============================] - 0s 144us/sample - loss: 0.6995 - accuracy: 0.9911\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a5f40c080>"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.fit(X_train,y_train,epochs=100,batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/38 [==============================] - 0s 2ms/sample - loss: 0.7148 - accuracy: 0.9211\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7147877028113917, 0.92105263]"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 딥러닝에서 overfitting을 막는 방식 \n",
    "# gradient vanishing 이라는 학습이 안 되는 문제가 발생 -> 이를 relu 로 해결했는데 이보다 더 나아가 새로운 방식을 발견함 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 층이 많으면 학습이 안 된다 => relu 로 해결 \n",
    "# 층이 많으면 많을 수록 overfitting 이 발생하는 문제를 아래의 4가지 방법으로 해결\n",
    "# => 기계학습에서는 L1, L2 방식으로 해결 (전통적)\n",
    "# => 딥러닝에서는 제프리 힌튼 교수가 고안한 방식으로 해결 (random하게 학습시킬 때마다 하나씩 제거) => drop out 방식 \n",
    "# => 딥러닝 EarlyStopping \n",
    "# => 딥러닝 Emsemble "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4 = Sequential()\n",
    "layer1 = Dense(16, input_shape=(4,), activation='relu')\n",
    "drop1 = Dropout(0.5) # 제프리 힌튼 교수가 제안한 방식 512개 중 256개만 들어감 (random하게 256개를 빼버림)\n",
    "layer2 = Dense(16, activation='relu')\n",
    "output_layer = Dense(3, activation='softmax')\n",
    "model4.add(layer1)\n",
    "model4.add(drop1)\n",
    "model4.add(layer2)\n",
    "model4.add(output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping() # 어느 정도 학습이 되었다고 생각하면 스스로 멈추어버리는 기능 -> model overfitting 방지 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\r",
      " 32/112 [=======>......................] - ETA: 0s - loss: 1.5029 - accuracy: 0.4062"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0424 17:12:41.019557 4538779072 callbacks.py:1004] Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 0s 2ms/sample - loss: 1.4928 - accuracy: 0.3036\n",
      "Epoch 2/100\n",
      " 32/112 [=======>......................] - ETA: 0s - loss: 1.5724 - accuracy: 0.2500"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0424 17:12:41.039961 4538779072 callbacks.py:1004] Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 0s 151us/sample - loss: 1.3680 - accuracy: 0.3125\n",
      "Epoch 3/100\n",
      " 32/112 [=======>......................] - ETA: 0s - loss: 1.4421 - accuracy: 0.2812"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0424 17:12:41.052544 4538779072 callbacks.py:1004] Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 0s 86us/sample - loss: 1.3445 - accuracy: 0.3750\n",
      "Epoch 4/100\n",
      " 32/112 [=======>......................] - ETA: 0s - loss: 1.3623 - accuracy: 0.3750"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0424 17:12:41.069831 4538779072 callbacks.py:1004] Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 0s 132us/sample - loss: 1.3038 - accuracy: 0.3482\n",
      "Epoch 5/100\n",
      " 32/112 [=======>......................] - ETA: 0s - loss: 1.2044 - accuracy: 0.3125"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0424 17:12:41.084125 4538779072 callbacks.py:1004] Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 0s 104us/sample - loss: 1.2472 - accuracy: 0.2857\n",
      "Epoch 6/100\n",
      " 32/112 [=======>......................] - ETA: 0s - loss: 1.3090 - accuracy: 0.2812"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0424 17:12:41.098587 4538779072 callbacks.py:1004] Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 0s 90us/sample - loss: 1.1640 - accuracy: 0.3750\n",
      "Epoch 7/100\n",
      " 32/112 [=======>......................] - ETA: 0s - loss: 1.0562 - accuracy: 0.3438"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0424 17:12:41.115554 4538779072 callbacks.py:1004] Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 0s 115us/sample - loss: 1.1208 - accuracy: 0.3571\n",
      "Epoch 8/100\n",
      " 32/112 [=======>......................] - ETA: 0s - loss: 1.2701 - accuracy: 0.2500"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0424 17:12:41.130707 4538779072 callbacks.py:1004] Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 0s 90us/sample - loss: 1.1869 - accuracy: 0.3929\n",
      "Epoch 9/100\n",
      " 32/112 [=======>......................] - ETA: 0s - loss: 1.2107 - accuracy: 0.2188"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0424 17:12:41.143019 4538779072 callbacks.py:1004] Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 0s 92us/sample - loss: 1.1620 - accuracy: 0.3125\n",
      "Epoch 10/100\n",
      " 32/112 [=======>......................] - ETA: 0s - loss: 1.1138 - accuracy: 0.4688"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0424 17:12:41.163930 4538779072 callbacks.py:1004] Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 0s 118us/sample - loss: 1.0519 - accuracy: 0.4286\n",
      "Epoch 11/100\n",
      " 32/112 [=======>......................] - ETA: 0s - loss: 1.2614 - accuracy: 0.3438"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0424 17:12:41.179961 4538779072 callbacks.py:1004] Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 0s 121us/sample - loss: 1.1082 - accuracy: 0.3750\n",
      "Epoch 12/100\n",
      " 32/112 [=======>......................] - ETA: 0s - loss: 1.0892 - accuracy: 0.4375"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0424 17:12:41.199109 4538779072 callbacks.py:1004] Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 0s 121us/sample - loss: 1.0156 - accuracy: 0.4464\n",
      "Epoch 13/100\n",
      " 32/112 [=======>......................] - ETA: 0s - loss: 0.9641 - accuracy: 0.4375"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0424 17:12:41.213890 4538779072 callbacks.py:1004] Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 0s 104us/sample - loss: 1.0083 - accuracy: 0.4554\n",
      "Epoch 14/100\n",
      " 32/112 [=======>......................] - ETA: 0s - loss: 1.1471 - accuracy: 0.3750"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0424 17:12:41.230734 4538779072 callbacks.py:1004] Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 0s 109us/sample - loss: 1.0552 - accuracy: 0.3929\n",
      "Epoch 15/100\n",
      " 32/112 [=======>......................] - ETA: 0s - loss: 1.0098 - accuracy: 0.5312"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0424 17:12:41.246683 4538779072 callbacks.py:1004] Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 0s 118us/sample - loss: 0.9965 - accuracy: 0.5179\n",
      "Epoch 16/100\n",
      " 32/112 [=======>......................] - ETA: 0s - loss: 0.9327 - accuracy: 0.4688"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0424 17:12:41.259781 4538779072 callbacks.py:1004] Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 0s 84us/sample - loss: 0.9814 - accuracy: 0.4107\n",
      "Epoch 17/100\n",
      " 32/112 [=======>......................] - ETA: 0s - loss: 0.9995 - accuracy: 0.4688"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0424 17:12:41.275827 4538779072 callbacks.py:1004] Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 0s 104us/sample - loss: 1.0017 - accuracy: 0.4375\n",
      "Epoch 18/100\n",
      " 32/112 [=======>......................] - ETA: 0s - loss: 0.9346 - accuracy: 0.5000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0424 17:12:41.289113 4538779072 callbacks.py:1004] Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 0s 109us/sample - loss: 0.8926 - accuracy: 0.5357\n",
      "Epoch 19/100\n",
      " 32/112 [=======>......................] - ETA: 0s - loss: 0.7666 - accuracy: 0.7188"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0424 17:12:41.310330 4538779072 callbacks.py:1004] Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 0s 115us/sample - loss: 0.8746 - accuracy: 0.5536\n",
      "Epoch 20/100\n",
      " 32/112 [=======>......................] - ETA: 0s - loss: 0.8254 - accuracy: 0.7812"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0424 17:12:41.322864 4538779072 callbacks.py:1004] Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 0s 79us/sample - loss: 0.8751 - accuracy: 0.6071\n",
      "Epoch 21/100\n",
      " 32/112 [=======>......................] - ETA: 0s - loss: 0.9111 - accuracy: 0.4688"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0424 17:12:41.336974 4538779072 callbacks.py:1004] Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 0s 107us/sample - loss: 0.8645 - accuracy: 0.5893\n",
      "Epoch 22/100\n",
      " 32/112 [=======>......................] - ETA: 0s - loss: 0.8916 - accuracy: 0.5000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0424 17:12:41.349722 4538779072 callbacks.py:1004] Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 0s 91us/sample - loss: 0.8127 - accuracy: 0.5982\n",
      "Epoch 23/100\n",
      " 32/112 [=======>......................] - ETA: 0s - loss: 0.8149 - accuracy: 0.6562"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0424 17:12:41.360959 4538779072 callbacks.py:1004] Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 0s 78us/sample - loss: 0.7861 - accuracy: 0.5982\n",
      "Epoch 24/100\n",
      " 32/112 [=======>......................] - ETA: 0s - loss: 0.6263 - accuracy: 0.8438"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0424 17:12:41.372019 4538779072 callbacks.py:1004] Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 0s 80us/sample - loss: 0.7078 - accuracy: 0.7054\n",
      "Epoch 25/100\n",
      " 32/112 [=======>......................] - ETA: 0s - loss: 0.8027 - accuracy: 0.5000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0424 17:12:41.381699 4538779072 callbacks.py:1004] Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 0s 69us/sample - loss: 0.7907 - accuracy: 0.5982\n",
      "Epoch 26/100\n",
      " 32/112 [=======>......................] - ETA: 0s - loss: 0.7299 - accuracy: 0.6250"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0424 17:12:41.391335 4538779072 callbacks.py:1004] Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 0s 71us/sample - loss: 0.7587 - accuracy: 0.6161\n",
      "Epoch 27/100\n",
      " 32/112 [=======>......................] - ETA: 0s - loss: 0.8173 - accuracy: 0.5312"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0424 17:12:41.402621 4538779072 callbacks.py:1004] Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 0s 80us/sample - loss: 0.8039 - accuracy: 0.5268\n",
      "Epoch 28/100\n",
      " 32/112 [=======>......................] - ETA: 0s - loss: 0.7269 - accuracy: 0.6250"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0424 17:12:41.416754 4538779072 callbacks.py:1004] Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 0s 105us/sample - loss: 0.7029 - accuracy: 0.6518\n",
      "Epoch 29/100\n",
      " 32/112 [=======>......................] - ETA: 0s - loss: 0.6862 - accuracy: 0.5938"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0424 17:12:41.431686 4538779072 callbacks.py:1004] Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 0s 116us/sample - loss: 0.6788 - accuracy: 0.6875\n",
      "Epoch 30/100\n",
      " 32/112 [=======>......................] - ETA: 0s - loss: 0.7806 - accuracy: 0.6250"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0424 17:12:41.445080 4538779072 callbacks.py:1004] Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 0s 102us/sample - loss: 0.6943 - accuracy: 0.6696\n",
      "Epoch 31/100\n",
      " 32/112 [=======>......................] - ETA: 0s - loss: 0.6936 - accuracy: 0.5312"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0424 17:12:41.457397 4538779072 callbacks.py:1004] Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 0s 93us/sample - loss: 0.6819 - accuracy: 0.6071\n",
      "Epoch 32/100\n",
      " 32/112 [=======>......................] - ETA: 0s - loss: 0.7720 - accuracy: 0.5625"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0424 17:12:41.474056 4538779072 callbacks.py:1004] Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 0s 124us/sample - loss: 0.7477 - accuracy: 0.5893\n",
      "Epoch 33/100\n",
      " 32/112 [=======>......................] - ETA: 0s - loss: 0.5867 - accuracy: 0.7188"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0424 17:12:41.489356 4538779072 callbacks.py:1004] Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 0s 113us/sample - loss: 0.6900 - accuracy: 0.6429\n",
      "Epoch 34/100\n",
      " 32/112 [=======>......................] - ETA: 0s - loss: 0.6294 - accuracy: 0.6875"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0424 17:12:41.516407 4538779072 callbacks.py:1004] Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 0s 204us/sample - loss: 0.6431 - accuracy: 0.6696\n",
      "Epoch 35/100\n",
      " 32/112 [=======>......................] - ETA: 0s - loss: 0.6240 - accuracy: 0.5938"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0424 17:12:41.537431 4538779072 callbacks.py:1004] Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 0s 174us/sample - loss: 0.7151 - accuracy: 0.5893\n",
      "Epoch 36/100\n",
      " 32/112 [=======>......................] - ETA: 0s - loss: 0.6004 - accuracy: 0.6250"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0424 17:12:41.552344 4538779072 callbacks.py:1004] Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 0s 93us/sample - loss: 0.6291 - accuracy: 0.7054\n",
      "Epoch 37/100\n",
      " 32/112 [=======>......................] - ETA: 0s - loss: 0.5641 - accuracy: 0.7188"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0424 17:12:41.566338 4538779072 callbacks.py:1004] Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 0s 115us/sample - loss: 0.5877 - accuracy: 0.7143\n",
      "Epoch 38/100\n",
      " 32/112 [=======>......................] - ETA: 0s - loss: 0.6042 - accuracy: 0.6875"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0424 17:12:41.579727 4538779072 callbacks.py:1004] Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 0s 96us/sample - loss: 0.6260 - accuracy: 0.7232\n",
      "Epoch 39/100\n",
      " 32/112 [=======>......................] - ETA: 0s - loss: 0.6359 - accuracy: 0.6562"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0424 17:12:41.593465 4538779072 callbacks.py:1004] Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 0s 108us/sample - loss: 0.6549 - accuracy: 0.6429\n",
      "Epoch 40/100\n",
      " 32/112 [=======>......................] - ETA: 0s - loss: 0.6656 - accuracy: 0.6250"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0424 17:12:41.606087 4538779072 callbacks.py:1004] Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 0s 85us/sample - loss: 0.6366 - accuracy: 0.6518\n",
      "Epoch 41/100\n",
      " 32/112 [=======>......................] - ETA: 0s - loss: 0.6707 - accuracy: 0.6562"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0424 17:12:41.618435 4538779072 callbacks.py:1004] Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 0s 92us/sample - loss: 0.6204 - accuracy: 0.6964\n",
      "Epoch 42/100\n",
      " 32/112 [=======>......................] - ETA: 0s - loss: 0.6643 - accuracy: 0.7500"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0424 17:12:41.632621 4538779072 callbacks.py:1004] Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 0s 114us/sample - loss: 0.6018 - accuracy: 0.6875\n",
      "Epoch 43/100\n",
      " 32/112 [=======>......................] - ETA: 0s - loss: 0.6737 - accuracy: 0.6562"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0424 17:12:41.646749 4538779072 callbacks.py:1004] Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 0s 99us/sample - loss: 0.6220 - accuracy: 0.7054\n",
      "Epoch 44/100\n",
      " 32/112 [=======>......................] - ETA: 0s - loss: 0.6776 - accuracy: 0.5000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0424 17:12:41.658833 4538779072 callbacks.py:1004] Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 0s 92us/sample - loss: 0.6054 - accuracy: 0.6964\n",
      "Epoch 45/100\n",
      " 32/112 [=======>......................] - ETA: 0s - loss: 0.6496 - accuracy: 0.5312"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0424 17:12:41.671199 4538779072 callbacks.py:1004] Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 0s 84us/sample - loss: 0.5943 - accuracy: 0.6607\n",
      "Epoch 46/100\n",
      " 32/112 [=======>......................] - ETA: 0s - loss: 0.5334 - accuracy: 0.8125"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0424 17:12:41.683685 4538779072 callbacks.py:1004] Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 0s 92us/sample - loss: 0.5780 - accuracy: 0.7411\n",
      "Epoch 47/100\n",
      " 32/112 [=======>......................] - ETA: 0s - loss: 0.6760 - accuracy: 0.6562"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0424 17:12:41.696977 4538779072 callbacks.py:1004] Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 0s 98us/sample - loss: 0.6171 - accuracy: 0.7589\n",
      "Epoch 48/100\n",
      " 32/112 [=======>......................] - ETA: 0s - loss: 0.6572 - accuracy: 0.6562"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0424 17:12:41.708331 4538779072 callbacks.py:1004] Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 0s 72us/sample - loss: 0.5914 - accuracy: 0.7411\n",
      "Epoch 49/100\n",
      " 32/112 [=======>......................] - ETA: 0s - loss: 0.4979 - accuracy: 0.7500"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0424 17:12:41.719985 4538779072 callbacks.py:1004] Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 0s 83us/sample - loss: 0.5557 - accuracy: 0.6696\n",
      "Epoch 50/100\n",
      " 32/112 [=======>......................] - ETA: 0s - loss: 0.6286 - accuracy: 0.5938"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0424 17:12:41.733572 4538779072 callbacks.py:1004] Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 0s 93us/sample - loss: 0.6052 - accuracy: 0.7054\n",
      "Epoch 51/100\n",
      " 32/112 [=======>......................] - ETA: 0s - loss: 0.4339 - accuracy: 0.8438"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0424 17:12:41.745082 4538779072 callbacks.py:1004] Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 0s 93us/sample - loss: 0.5072 - accuracy: 0.8036\n",
      "Epoch 52/100\n",
      " 32/112 [=======>......................] - ETA: 0s - loss: 0.5903 - accuracy: 0.6562"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0424 17:12:41.757913 4538779072 callbacks.py:1004] Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 0s 84us/sample - loss: 0.5998 - accuracy: 0.7143\n",
      "Epoch 53/100\n",
      " 32/112 [=======>......................] - ETA: 0s - loss: 0.5207 - accuracy: 0.6875"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0424 17:12:41.768918 4538779072 callbacks.py:1004] Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 0s 73us/sample - loss: 0.5890 - accuracy: 0.6518\n",
      "Epoch 54/100\n",
      " 32/112 [=======>......................] - ETA: 0s - loss: 0.5281 - accuracy: 0.8125"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0424 17:12:41.784731 4538779072 callbacks.py:1004] Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 0s 104us/sample - loss: 0.5752 - accuracy: 0.7143\n",
      "Epoch 55/100\n",
      " 32/112 [=======>......................] - ETA: 0s - loss: 0.5734 - accuracy: 0.7812"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0424 17:12:41.795818 4538779072 callbacks.py:1004] Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 0s 85us/sample - loss: 0.6267 - accuracy: 0.6250\n",
      "Epoch 56/100\n",
      " 32/112 [=======>......................] - ETA: 0s - loss: 0.4995 - accuracy: 0.7812"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0424 17:12:41.810661 4538779072 callbacks.py:1004] Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 0s 140us/sample - loss: 0.5319 - accuracy: 0.7411\n",
      "Epoch 57/100\n",
      " 32/112 [=======>......................] - ETA: 0s - loss: 0.5495 - accuracy: 0.7188"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0424 17:12:41.828977 4538779072 callbacks.py:1004] Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 0s 106us/sample - loss: 0.5974 - accuracy: 0.6964\n",
      "Epoch 58/100\n",
      " 32/112 [=======>......................] - ETA: 0s - loss: 0.5065 - accuracy: 0.8125"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0424 17:12:41.839570 4538779072 callbacks.py:1004] Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 0s 74us/sample - loss: 0.5284 - accuracy: 0.7768\n",
      "Epoch 59/100\n",
      " 32/112 [=======>......................] - ETA: 0s - loss: 0.5268 - accuracy: 0.7812"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0424 17:12:41.853836 4538779072 callbacks.py:1004] Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 0s 97us/sample - loss: 0.5120 - accuracy: 0.7411\n",
      "Epoch 60/100\n",
      " 32/112 [=======>......................] - ETA: 0s - loss: 0.4407 - accuracy: 0.7188"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0424 17:12:41.863744 4538779072 callbacks.py:1004] Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 0s 68us/sample - loss: 0.5204 - accuracy: 0.7321\n",
      "Epoch 61/100\n",
      " 32/112 [=======>......................] - ETA: 0s - loss: 0.5891 - accuracy: 0.6562"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0424 17:12:41.875783 4538779072 callbacks.py:1004] Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 0s 95us/sample - loss: 0.5194 - accuracy: 0.7321\n",
      "Epoch 62/100\n",
      " 32/112 [=======>......................] - ETA: 0s - loss: 0.5563 - accuracy: 0.7188"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0424 17:12:41.888143 4538779072 callbacks.py:1004] Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 0s 88us/sample - loss: 0.5876 - accuracy: 0.7143\n",
      "Epoch 63/100\n",
      " 32/112 [=======>......................] - ETA: 0s - loss: 0.5428 - accuracy: 0.7812"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0424 17:12:41.898175 4538779072 callbacks.py:1004] Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 0s 75us/sample - loss: 0.5278 - accuracy: 0.7411\n",
      "Epoch 64/100\n",
      " 32/112 [=======>......................] - ETA: 0s - loss: 0.6389 - accuracy: 0.6562"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0424 17:12:41.909111 4538779072 callbacks.py:1004] Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 0s 89us/sample - loss: 0.5160 - accuracy: 0.7589\n",
      "Epoch 65/100\n",
      " 32/112 [=======>......................] - ETA: 0s - loss: 0.5288 - accuracy: 0.6562"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0424 17:12:41.920267 4538779072 callbacks.py:1004] Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 0s 67us/sample - loss: 0.5315 - accuracy: 0.7232\n",
      "Epoch 66/100\n",
      " 32/112 [=======>......................] - ETA: 0s - loss: 0.6056 - accuracy: 0.6562"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0424 17:12:41.932013 4538779072 callbacks.py:1004] Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 0s 85us/sample - loss: 0.5151 - accuracy: 0.7500\n",
      "Epoch 67/100\n",
      " 32/112 [=======>......................] - ETA: 0s - loss: 0.5314 - accuracy: 0.7812"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0424 17:12:41.945556 4538779072 callbacks.py:1004] Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 0s 83us/sample - loss: 0.5182 - accuracy: 0.7589\n",
      "Epoch 68/100\n",
      " 32/112 [=======>......................] - ETA: 0s - loss: 0.5001 - accuracy: 0.8125"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0424 17:12:41.956918 4538779072 callbacks.py:1004] Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 0s 83us/sample - loss: 0.5054 - accuracy: 0.7500\n",
      "Epoch 69/100\n",
      " 32/112 [=======>......................] - ETA: 0s - loss: 0.4495 - accuracy: 0.8438"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0424 17:12:41.966621 4538779072 callbacks.py:1004] Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 0s 70us/sample - loss: 0.5632 - accuracy: 0.7054\n",
      "Epoch 70/100\n",
      " 32/112 [=======>......................] - ETA: 0s - loss: 0.5232 - accuracy: 0.7812"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0424 17:12:41.976618 4538779072 callbacks.py:1004] Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 0s 76us/sample - loss: 0.5585 - accuracy: 0.7321\n",
      "Epoch 71/100\n",
      " 32/112 [=======>......................] - ETA: 0s - loss: 0.5011 - accuracy: 0.7812"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0424 17:12:41.988738 4538779072 callbacks.py:1004] Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 0s 90us/sample - loss: 0.5023 - accuracy: 0.7500\n",
      "Epoch 72/100\n",
      " 32/112 [=======>......................] - ETA: 0s - loss: 0.5246 - accuracy: 0.6875"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0424 17:12:42.005656 4538779072 callbacks.py:1004] Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 0s 106us/sample - loss: 0.5419 - accuracy: 0.7143\n",
      "Epoch 73/100\n",
      " 32/112 [=======>......................] - ETA: 0s - loss: 0.4408 - accuracy: 0.8750"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0424 17:12:42.015887 4538779072 callbacks.py:1004] Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 0s 70us/sample - loss: 0.4989 - accuracy: 0.7768\n",
      "Epoch 74/100\n",
      " 32/112 [=======>......................] - ETA: 0s - loss: 0.4961 - accuracy: 0.6562"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0424 17:12:42.027573 4538779072 callbacks.py:1004] Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 0s 81us/sample - loss: 0.4749 - accuracy: 0.7857\n",
      "Epoch 75/100\n",
      " 32/112 [=======>......................] - ETA: 0s - loss: 0.5316 - accuracy: 0.7500"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0424 17:12:42.039165 4538779072 callbacks.py:1004] Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 0s 87us/sample - loss: 0.4988 - accuracy: 0.7679\n",
      "Epoch 76/100\n",
      " 32/112 [=======>......................] - ETA: 0s - loss: 0.5496 - accuracy: 0.7812"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0424 17:12:42.050993 4538779072 callbacks.py:1004] Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 0s 88us/sample - loss: 0.5579 - accuracy: 0.7054\n",
      "Epoch 77/100\n",
      " 32/112 [=======>......................] - ETA: 0s - loss: 0.4412 - accuracy: 0.8125"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0424 17:12:42.065918 4538779072 callbacks.py:1004] Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 0s 108us/sample - loss: 0.4847 - accuracy: 0.7768\n",
      "Epoch 78/100\n",
      " 32/112 [=======>......................] - ETA: 0s - loss: 0.4089 - accuracy: 0.7500"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0424 17:12:42.076913 4538779072 callbacks.py:1004] Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 0s 73us/sample - loss: 0.4751 - accuracy: 0.7589\n",
      "Epoch 79/100\n",
      " 32/112 [=======>......................] - ETA: 0s - loss: 0.5820 - accuracy: 0.7500"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0424 17:12:42.086945 4538779072 callbacks.py:1004] Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 0s 69us/sample - loss: 0.5000 - accuracy: 0.8214\n",
      "Epoch 80/100\n",
      " 32/112 [=======>......................] - ETA: 0s - loss: 0.4581 - accuracy: 0.8750"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0424 17:12:42.098822 4538779072 callbacks.py:1004] Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 0s 82us/sample - loss: 0.4962 - accuracy: 0.7946\n",
      "Epoch 81/100\n",
      " 32/112 [=======>......................] - ETA: 0s - loss: 0.5104 - accuracy: 0.7812"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0424 17:12:42.111712 4538779072 callbacks.py:1004] Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 0s 93us/sample - loss: 0.5277 - accuracy: 0.8125\n",
      "Epoch 82/100\n",
      " 32/112 [=======>......................] - ETA: 0s - loss: 0.5610 - accuracy: 0.8125"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0424 17:12:42.125413 4538779072 callbacks.py:1004] Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 0s 96us/sample - loss: 0.4653 - accuracy: 0.8393\n",
      "Epoch 83/100\n",
      " 32/112 [=======>......................] - ETA: 0s - loss: 0.5768 - accuracy: 0.7812"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0424 17:12:42.136081 4538779072 callbacks.py:1004] Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 0s 76us/sample - loss: 0.4815 - accuracy: 0.7857\n",
      "Epoch 84/100\n",
      " 32/112 [=======>......................] - ETA: 0s - loss: 0.4152 - accuracy: 0.8438"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0424 17:12:42.148037 4538779072 callbacks.py:1004] Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 0s 91us/sample - loss: 0.4608 - accuracy: 0.8214\n",
      "Epoch 85/100\n",
      " 32/112 [=======>......................] - ETA: 0s - loss: 0.5281 - accuracy: 0.7500"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0424 17:12:42.162400 4538779072 callbacks.py:1004] Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 0s 98us/sample - loss: 0.5446 - accuracy: 0.7411\n",
      "Epoch 86/100\n",
      " 32/112 [=======>......................] - ETA: 0s - loss: 0.5097 - accuracy: 0.8125"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0424 17:12:42.174886 4538779072 callbacks.py:1004] Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 0s 89us/sample - loss: 0.5136 - accuracy: 0.7679\n",
      "Epoch 87/100\n",
      " 32/112 [=======>......................] - ETA: 0s - loss: 0.4943 - accuracy: 0.8125"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0424 17:12:42.186264 4538779072 callbacks.py:1004] Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 0s 83us/sample - loss: 0.4422 - accuracy: 0.8214\n",
      "Epoch 88/100\n",
      " 32/112 [=======>......................] - ETA: 0s - loss: 0.5899 - accuracy: 0.7188"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0424 17:12:42.197678 4538779072 callbacks.py:1004] Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 0s 81us/sample - loss: 0.5421 - accuracy: 0.7679\n",
      "Epoch 89/100\n",
      " 32/112 [=======>......................] - ETA: 0s - loss: 0.4919 - accuracy: 0.7812"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0424 17:12:42.207970 4538779072 callbacks.py:1004] Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 0s 71us/sample - loss: 0.4674 - accuracy: 0.7768\n",
      "Epoch 90/100\n",
      " 32/112 [=======>......................] - ETA: 0s - loss: 0.5593 - accuracy: 0.6562"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0424 17:12:42.217948 4538779072 callbacks.py:1004] Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 0s 68us/sample - loss: 0.5001 - accuracy: 0.7679\n",
      "Epoch 91/100\n",
      " 32/112 [=======>......................] - ETA: 0s - loss: 0.5708 - accuracy: 0.7500"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0424 17:12:42.228961 4538779072 callbacks.py:1004] Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 0s 82us/sample - loss: 0.4948 - accuracy: 0.8125\n",
      "Epoch 92/100\n",
      " 32/112 [=======>......................] - ETA: 0s - loss: 0.4940 - accuracy: 0.7500"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0424 17:12:42.239624 4538779072 callbacks.py:1004] Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 0s 77us/sample - loss: 0.4617 - accuracy: 0.7946\n",
      "Epoch 93/100\n",
      " 32/112 [=======>......................] - ETA: 0s - loss: 0.4379 - accuracy: 0.7188"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0424 17:12:42.251300 4538779072 callbacks.py:1004] Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 0s 80us/sample - loss: 0.4849 - accuracy: 0.7679\n",
      "Epoch 94/100\n",
      " 32/112 [=======>......................] - ETA: 0s - loss: 0.4356 - accuracy: 0.8125"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0424 17:12:42.262181 4538779072 callbacks.py:1004] Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 0s 72us/sample - loss: 0.4975 - accuracy: 0.7946\n",
      "Epoch 95/100\n",
      " 32/112 [=======>......................] - ETA: 0s - loss: 0.5496 - accuracy: 0.7812"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0424 17:12:42.273624 4538779072 callbacks.py:1004] Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 0s 88us/sample - loss: 0.5217 - accuracy: 0.7946\n",
      "Epoch 96/100\n",
      " 32/112 [=======>......................] - ETA: 0s - loss: 0.4610 - accuracy: 0.8125"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0424 17:12:42.288346 4538779072 callbacks.py:1004] Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 0s 102us/sample - loss: 0.4884 - accuracy: 0.7589\n",
      "Epoch 97/100\n",
      " 32/112 [=======>......................] - ETA: 0s - loss: 0.5426 - accuracy: 0.7188"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0424 17:12:42.302194 4538779072 callbacks.py:1004] Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 0s 99us/sample - loss: 0.4621 - accuracy: 0.7857\n",
      "Epoch 98/100\n",
      " 32/112 [=======>......................] - ETA: 0s - loss: 0.5023 - accuracy: 0.6875"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0424 17:12:42.316646 4538779072 callbacks.py:1004] Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 0s 93us/sample - loss: 0.4884 - accuracy: 0.7500\n",
      "Epoch 99/100\n",
      " 32/112 [=======>......................] - ETA: 0s - loss: 0.5477 - accuracy: 0.7812"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0424 17:12:42.327429 4538779072 callbacks.py:1004] Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 0s 80us/sample - loss: 0.4793 - accuracy: 0.8125\n",
      "Epoch 100/100\n",
      " 32/112 [=======>......................] - ETA: 0s - loss: 0.4855 - accuracy: 0.8438"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0424 17:12:42.338119 4538779072 callbacks.py:1004] Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "112/112 [==============================] - 0s 76us/sample - loss: 0.4634 - accuracy: 0.8304\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a644db8d0>"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model4.fit(X_train,y_train,epochs=100,batch_size=32,callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/38 [==============================] - 0s 2ms/sample - loss: 0.3275 - accuracy: 0.9737\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3275289457095297, 0.9736842]"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model4.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_26\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_81 (Dense)             (None, 16)                80        \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_82 (Dense)             (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_83 (Dense)             (None, 3)                 51        \n",
      "=================================================================\n",
      "Total params: 403\n",
      "Trainable params: 403\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model4.save_weights() # 가중치 (weight) 만 저장 -> 학습시키는 것은 weight 이기 때문에 중요하다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4.save('moon.h5') # model4 자체를 저장 => 객체를 저장 => pickling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "model5 = load_model('moon.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_obj_reference_counts_dict': <tensorflow.python.training.tracking.object_identity.ObjectIdentityDictionary at 0x1a652474e0>,\n",
       " '_name': 'sequential_26',\n",
       " '_activity_regularizer': None,\n",
       " 'trainable': True,\n",
       " '_is_compiled': True,\n",
       " '_expects_training_arg': True,\n",
       " '_compute_output_and_mask_jointly': True,\n",
       " 'supports_masking': False,\n",
       " 'optimizer': <tensorflow.python.keras.optimizer_v2.adam.Adam at 0x1a65247b38>,\n",
       " '_trainable_weights': [],\n",
       " '_non_trainable_weights': [],\n",
       " '_updates': [],\n",
       " '_losses': [],\n",
       " '_eager_losses': [],\n",
       " '_metrics': [],\n",
       " '_metrics_tensors': {},\n",
       " '_scope': None,\n",
       " '_reuse': None,\n",
       " '_graph': None,\n",
       " '_dtype': None,\n",
       " '_layers': [<tensorflow.python.keras.engine.input_layer.InputLayer at 0x1a6532bcc0>,\n",
       "  <tensorflow.python.keras.layers.core.Dense at 0x1a653179e8>,\n",
       "  <tensorflow.python.keras.layers.core.Dropout at 0x1a65317da0>,\n",
       "  <tensorflow.python.keras.layers.core.Dense at 0x1a653261d0>,\n",
       "  <tensorflow.python.keras.layers.core.Dense at 0x1a65359a90>],\n",
       " '_outbound_nodes': [],\n",
       " '_inbound_nodes': [<tensorflow.python.keras.engine.base_layer.Node at 0x1a65329a90>],\n",
       " '_trackable_saver': <tensorflow.python.training.tracking.util.TrackableSaver at 0x1a65359e80>,\n",
       " '_mixed_precision_policy': <tensorflow.python.keras.mixed_precision.experimental.policy.Policy at 0x1a65247cf8>,\n",
       " '_is_graph_network': True,\n",
       " '_dynamic': False,\n",
       " '_call_convention': <CallConvention.EXPLICIT_INPUTS_ARGUMENT: 1>,\n",
       " 'outputs': [<tf.Tensor 'dense_83_1/Softmax:0' shape=(None, 3) dtype=float32>],\n",
       " 'inputs': [<tf.Tensor 'dense_81_input_1:0' shape=(None, 4) dtype=float32>],\n",
       " 'built': True,\n",
       " '_distribution_strategy': None,\n",
       " '_compile_distribution': False,\n",
       " '_run_eagerly': None,\n",
       " '_build_input_shape': None,\n",
       " '_layer_call_argspecs': {<tensorflow.python.keras.engine.input_layer.InputLayer at 0x1a6532bcc0>: FullArgSpec(args=['self', 'inputs'], varargs=None, varkw='kwargs', defaults=None, kwonlyargs=[], kwonlydefaults=None, annotations={}),\n",
       "  <tensorflow.python.keras.layers.core.Dense at 0x1a653179e8>: FullArgSpec(args=['self', 'inputs'], varargs=None, varkw=None, defaults=None, kwonlyargs=[], kwonlydefaults=None, annotations={}),\n",
       "  <tensorflow.python.keras.layers.core.Dropout at 0x1a65317da0>: FullArgSpec(args=['self', 'inputs', 'training'], varargs=None, varkw=None, defaults=(None,), kwonlyargs=[], kwonlydefaults=None, annotations={}),\n",
       "  <tensorflow.python.keras.layers.core.Dense at 0x1a653261d0>: FullArgSpec(args=['self', 'inputs'], varargs=None, varkw=None, defaults=None, kwonlyargs=[], kwonlydefaults=None, annotations={}),\n",
       "  <tensorflow.python.keras.layers.core.Dense at 0x1a65359a90>: FullArgSpec(args=['self', 'inputs'], varargs=None, varkw=None, defaults=None, kwonlyargs=[], kwonlydefaults=None, annotations={})},\n",
       " '_setattr_tracking': True,\n",
       " '_nested_outputs': <tf.Tensor 'dense_83_1/Softmax:0' shape=(None, 3) dtype=float32>,\n",
       " '_nested_inputs': <tf.Tensor 'dense_81_input_1:0' shape=(None, 4) dtype=float32>,\n",
       " '_compute_previous_mask': True,\n",
       " '_input_layers': [<tensorflow.python.keras.engine.input_layer.InputLayer at 0x1a6532bcc0>],\n",
       " '_output_layers': [<tensorflow.python.keras.layers.core.Dense at 0x1a65359a90>],\n",
       " '_input_coordinates': [(<tensorflow.python.keras.engine.input_layer.InputLayer at 0x1a6532bcc0>,\n",
       "   0,\n",
       "   0)],\n",
       " '_output_coordinates': [(<tensorflow.python.keras.layers.core.Dense at 0x1a65359a90>,\n",
       "   0,\n",
       "   0)],\n",
       " '_output_mask_cache': {},\n",
       " '_output_tensor_cache': {},\n",
       " '_output_shape_cache': {},\n",
       " '_network_nodes': {'dense_81_ib-0',\n",
       "  'dense_81_input_ib-0',\n",
       "  'dense_82_ib-0',\n",
       "  'dense_83_ib-0',\n",
       "  'dropout_8_ib-0'},\n",
       " '_nodes_by_depth': {0: [<tensorflow.python.keras.engine.base_layer.Node at 0x1a65373400>],\n",
       "  1: [<tensorflow.python.keras.engine.base_layer.Node at 0x1a65359470>],\n",
       "  2: [<tensorflow.python.keras.engine.base_layer.Node at 0x1a64926ef0>],\n",
       "  3: [<tensorflow.python.keras.engine.base_layer.Node at 0x1a64f2eeb8>],\n",
       "  4: [<tensorflow.python.keras.engine.base_layer.Node at 0x1a65317c18>]},\n",
       " '_layers_by_depth': {0: [<tensorflow.python.keras.layers.core.Dense at 0x1a65359a90>],\n",
       "  1: [<tensorflow.python.keras.layers.core.Dense at 0x1a653261d0>],\n",
       "  2: [<tensorflow.python.keras.layers.core.Dropout at 0x1a65317da0>],\n",
       "  3: [<tensorflow.python.keras.layers.core.Dense at 0x1a653179e8>],\n",
       "  4: [<tensorflow.python.keras.engine.input_layer.InputLayer at 0x1a6532bcc0>]},\n",
       " '_unconditional_checkpoint_dependencies': [TrackableReference(name='layer-0', ref=<tensorflow.python.keras.engine.input_layer.InputLayer object at 0x1a6532bcc0>),\n",
       "  TrackableReference(name='layer_with_weights-0', ref=<tensorflow.python.keras.layers.core.Dense object at 0x1a653179e8>),\n",
       "  TrackableReference(name='layer-1', ref=<tensorflow.python.keras.layers.core.Dense object at 0x1a653179e8>),\n",
       "  TrackableReference(name='layer-2', ref=<tensorflow.python.keras.layers.core.Dropout object at 0x1a65317da0>),\n",
       "  TrackableReference(name='layer_with_weights-1', ref=<tensorflow.python.keras.layers.core.Dense object at 0x1a653261d0>),\n",
       "  TrackableReference(name='layer-3', ref=<tensorflow.python.keras.layers.core.Dense object at 0x1a653261d0>),\n",
       "  TrackableReference(name='layer_with_weights-2', ref=<tensorflow.python.keras.layers.core.Dense object at 0x1a65359a90>),\n",
       "  TrackableReference(name='layer-4', ref=<tensorflow.python.keras.layers.core.Dense object at 0x1a65359a90>),\n",
       "  TrackableReference(name='optimizer', ref=<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x1a65247b38>)],\n",
       " '_unconditional_dependency_names': {'layer-0': <tensorflow.python.keras.engine.input_layer.InputLayer at 0x1a6532bcc0>,\n",
       "  'layer_with_weights-0': <tensorflow.python.keras.layers.core.Dense at 0x1a653179e8>,\n",
       "  'layer-1': <tensorflow.python.keras.layers.core.Dense at 0x1a653179e8>,\n",
       "  'layer-2': <tensorflow.python.keras.layers.core.Dropout at 0x1a65317da0>,\n",
       "  'layer_with_weights-1': <tensorflow.python.keras.layers.core.Dense at 0x1a653261d0>,\n",
       "  'layer-3': <tensorflow.python.keras.layers.core.Dense at 0x1a653261d0>,\n",
       "  'layer_with_weights-2': <tensorflow.python.keras.layers.core.Dense at 0x1a65359a90>,\n",
       "  'layer-4': <tensorflow.python.keras.layers.core.Dense at 0x1a65359a90>,\n",
       "  'optimizer': <tensorflow.python.keras.optimizer_v2.adam.Adam at 0x1a65247b38>},\n",
       " '_unconditional_deferred_dependencies': {},\n",
       " '_update_uid': -1,\n",
       " '_name_based_restores': set(),\n",
       " 'input_names': ['dense_81_input'],\n",
       " 'output_names': ['dense_83'],\n",
       " '_feed_input_names': ['dense_81_input'],\n",
       " '_feed_inputs': [<tf.Tensor 'dense_81_input_1:0' shape=(None, 4) dtype=float32>],\n",
       " '_feed_input_shapes': [(None, 4)],\n",
       " 'loss': 'categorical_crossentropy',\n",
       " '_compile_metrics': ['accuracy'],\n",
       " 'loss_weights': None,\n",
       " 'sample_weight_mode': None,\n",
       " '_compile_weighted_metrics': None,\n",
       " 'target_tensors': None,\n",
       " '_distributed_model_cache': {},\n",
       " '_compile_metrics_names': ['loss', 'accuracy'],\n",
       " '_compile_stateful_metric_functions': [<tensorflow.python.keras.metrics.MeanMetricWrapper at 0x1a653839e8>],\n",
       " '_compile_stateful_metrics_tensors': {'accuracy': <tf.Tensor 'metrics_26/accuracy/Identity:0' shape=() dtype=float32>},\n",
       " '_compile_metrics_tensors': {'accuracy': <tf.Tensor 'metrics_26/accuracy/Mean:0' shape=() dtype=float32>},\n",
       " '_output_loss_metrics': None,\n",
       " 'loss_functions': [<tensorflow.python.keras.losses.LossFunctionWrapper at 0x1a65373dd8>],\n",
       " '_feed_outputs': [<tf.Tensor 'dense_83_1/Softmax:0' shape=(None, 3) dtype=float32>],\n",
       " '_feed_output_names': ['dense_83'],\n",
       " '_feed_output_shapes': [(None, 3)],\n",
       " '_feed_loss_fns': [<tensorflow.python.keras.losses.LossFunctionWrapper at 0x1a65373dd8>],\n",
       " 'loss_weights_list': [1.0],\n",
       " 'targets': [<tf.Tensor 'dense_83_target_1:0' shape=(None, None) dtype=float32>],\n",
       " '_feed_targets': [<tf.Tensor 'dense_83_target_1:0' shape=(None, None) dtype=float32>],\n",
       " 'sample_weights': [<tf.Tensor 'dense_83_sample_weights_1:0' shape=(None,) dtype=float32>],\n",
       " 'sample_weight_modes': [None],\n",
       " '_feed_sample_weight_modes': [None],\n",
       " '_feed_sample_weights': [<tf.Tensor 'dense_83_sample_weights_1:0' shape=(None,) dtype=float32>],\n",
       " '_per_output_metrics': [OrderedDict([('accuracy',\n",
       "                (<function tensorflow.python.keras.metrics.categorical_accuracy(y_true, y_pred)>,\n",
       "                 <tensorflow.python.keras.metrics.MeanMetricWrapper at 0x1a653839e8>))])],\n",
       " '_per_output_weighted_metrics': [OrderedDict()],\n",
       " 'total_loss': <tf.Tensor 'loss_26/mul:0' shape=() dtype=float32>,\n",
       " '_function_kwargs': {},\n",
       " '_fit_function': None,\n",
       " '_eval_function': None,\n",
       " '_predict_function': None,\n",
       " 'train_function': <tensorflow.python.keras.backend.EagerExecutionFunction at 0x1a65383f60>,\n",
       " 'test_function': None,\n",
       " 'predict_function': None,\n",
       " '_collected_trainable_weights': [<tf.Variable 'dense_81_1/kernel:0' shape=(4, 16) dtype=float32, numpy=\n",
       "  array([[-0.40497375,  0.41353968,  0.25978062, -0.49297976, -0.4934385 ,\n",
       "          -0.42709157,  0.13684665, -0.08341065, -0.47127354,  0.01879375,\n",
       "          -0.1691321 , -0.10328475, -0.43005437, -0.18669353,  0.06605607,\n",
       "          -0.02524691],\n",
       "         [ 0.3735357 , -0.03946725, -0.5859534 , -0.3288534 , -0.01364285,\n",
       "           0.38550347, -0.19601293, -0.08362532,  0.5444881 , -0.15490364,\n",
       "           0.4810578 ,  0.5778352 ,  0.19301516,  0.27992982, -0.32238042,\n",
       "          -0.56976897],\n",
       "         [ 0.32140744, -0.48761314,  0.26442942, -0.26761895,  0.29397643,\n",
       "          -0.09728619, -0.11394004, -0.44859824, -0.03092766,  0.31760424,\n",
       "           0.37162757, -0.24448894,  0.41871116,  0.3505939 ,  0.3620376 ,\n",
       "           0.47490966],\n",
       "         [ 0.6806503 , -0.4383214 ,  0.4330565 ,  0.47176874, -0.2905117 ,\n",
       "           0.35707563,  0.39897612,  0.0544647 , -0.52208924,  0.2944462 ,\n",
       "          -0.6778998 , -0.0821063 ,  0.21308838,  0.3296871 ,  0.0419701 ,\n",
       "          -0.06637757]], dtype=float32)>,\n",
       "  <tf.Variable 'dense_81_1/bias:0' shape=(16,) dtype=float32, numpy=\n",
       "  array([-0.10054817,  0.03161494, -0.12250648,  0.        ,  0.        ,\n",
       "          0.        ,  0.03670643,  0.        ,  0.        , -0.14431626,\n",
       "          0.02438825,  0.14104016, -0.04762989, -0.10495262,  0.05382717,\n",
       "         -0.0823044 ], dtype=float32)>,\n",
       "  <tf.Variable 'dense_82_1/kernel:0' shape=(16, 16) dtype=float32, numpy=\n",
       "  array([[ 0.59610623,  0.10526135, -0.4017224 , -0.47047836, -0.3665879 ,\n",
       "          -0.13000254, -0.21666583,  0.37419695, -0.34529716,  0.02330822,\n",
       "           0.30044758,  0.03224415,  0.2141602 , -0.4262502 , -0.33088306,\n",
       "           0.035689  ],\n",
       "         [-0.3356248 ,  0.27561072,  0.10703624, -0.3259282 ,  0.22439662,\n",
       "          -0.43481678,  0.5803819 , -0.42574328,  0.27436268,  0.24494195,\n",
       "           0.5796693 ,  0.05252019, -0.3346863 ,  0.12237857,  0.45706168,\n",
       "           0.00340852],\n",
       "         [ 0.13178062,  0.10781183, -0.36481196,  0.28270468, -0.3383921 ,\n",
       "           0.0643519 , -0.1019186 ,  0.07877525,  0.0413058 ,  0.29005527,\n",
       "          -0.2041516 , -0.2869512 ,  0.00073483, -0.12521736, -0.3502919 ,\n",
       "           0.03571841],\n",
       "         [ 0.01978612,  0.17142841, -0.37074092,  0.42288014, -0.27203906,\n",
       "          -0.03574264, -0.36865538,  0.2642682 ,  0.16362056,  0.34124854,\n",
       "          -0.30462217,  0.3974779 , -0.04618582, -0.08939248, -0.4187812 ,\n",
       "           0.19757977],\n",
       "         [ 0.21263948,  0.02387404, -0.09959108, -0.27980927,  0.20546296,\n",
       "          -0.3634041 ,  0.22382924,  0.19893518,  0.3440695 ,  0.14065883,\n",
       "           0.03581676, -0.34121746, -0.09876609, -0.31564048,  0.05832621,\n",
       "           0.30542   ],\n",
       "         [ 0.15184215, -0.01422593, -0.2737757 ,  0.12309524, -0.32849407,\n",
       "           0.21381989,  0.090471  ,  0.2568302 , -0.35658768, -0.3532982 ,\n",
       "          -0.12189922,  0.03692171, -0.35541975,  0.20090583, -0.03853968,\n",
       "           0.3003039 ],\n",
       "         [-0.08774726, -0.09677627, -0.20575638, -0.06587084,  0.07090919,\n",
       "           0.2092341 , -0.09596027, -0.12246596,  0.28352153, -0.01796189,\n",
       "           0.3345692 ,  0.21672398,  0.568722  ,  0.30818352, -0.09429135,\n",
       "          -0.2853538 ],\n",
       "         [ 0.12652388, -0.38716066,  0.08443257, -0.37647992, -0.05877271,\n",
       "           0.42732683,  0.39764997, -0.15767705, -0.03846917, -0.12713918,\n",
       "          -0.00506106,  0.344022  , -0.11900771, -0.3452263 ,  0.28461096,\n",
       "          -0.09065703],\n",
       "         [ 0.11118773, -0.41497087, -0.27976385, -0.12882134, -0.37772727,\n",
       "           0.1008738 , -0.28505677, -0.05845702,  0.06271458, -0.1430966 ,\n",
       "          -0.24122325, -0.00411901, -0.1580225 ,  0.08262625, -0.24570689,\n",
       "           0.20504227],\n",
       "         [-0.16299866,  0.14159152, -0.38927004, -0.25288218,  0.1799804 ,\n",
       "           0.02320625, -0.29714578,  0.09203105, -0.29093775,  0.09921391,\n",
       "          -0.09660219, -0.16633268,  0.33713588,  0.09140354,  0.19437382,\n",
       "           0.07215485],\n",
       "         [-0.13526255, -0.29631767,  0.3909337 ,  0.10031395,  0.34928763,\n",
       "           0.29442936,  0.06458551,  0.37816784,  0.11781979, -0.2997812 ,\n",
       "          -0.2031057 ,  0.32776394,  0.12922688, -0.25404915,  0.26177406,\n",
       "          -0.40772104],\n",
       "         [ 0.19633406,  0.05650317,  0.6132832 , -0.04356859,  0.27489495,\n",
       "          -0.02586743,  0.55823296, -0.16418348, -0.07441904, -0.35612047,\n",
       "           0.60244316,  0.5492653 ,  0.04497098, -0.4914771 ,  0.6059069 ,\n",
       "           0.18030739],\n",
       "         [ 0.11169601,  0.28706348, -0.13460027, -0.17994407,  0.09254765,\n",
       "          -0.3867649 , -0.2633404 ,  0.63348573, -0.36945313,  0.48974812,\n",
       "          -0.41763547,  0.11455102,  0.10819127,  0.11366063,  0.04926859,\n",
       "          -0.39970618],\n",
       "         [ 0.37690642,  0.30941993, -0.14181764, -0.15299553, -0.06330179,\n",
       "           0.11990055,  0.18943009,  0.2564462 ,  0.16382398, -0.18796372,\n",
       "           0.03482207, -0.28400728, -0.24084458,  0.28948113,  0.1659674 ,\n",
       "           0.13406076],\n",
       "         [ 0.5823926 ,  0.27261022,  0.03319226,  0.09712312,  0.29190323,\n",
       "           0.20702356,  0.16160275, -0.14144944, -0.22278269,  0.36660394,\n",
       "          -0.3019446 , -0.52789104,  0.5855228 , -0.23736168, -0.41888103,\n",
       "           0.1599679 ],\n",
       "         [ 0.27034262, -0.19605637,  0.34522477, -0.183     ,  0.43232226,\n",
       "          -0.07901031,  0.23517078,  0.00574919,  0.01580358,  0.45436093,\n",
       "          -0.31398842,  0.27364954, -0.19432168,  0.18476796, -0.07714664,\n",
       "          -0.4549319 ]], dtype=float32)>,\n",
       "  <tf.Variable 'dense_82_1/bias:0' shape=(16,) dtype=float32, numpy=\n",
       "  array([-0.012439  , -0.12861644,  0.13481642, -0.1321435 , -0.08006474,\n",
       "          0.1301741 ,  0.20983186, -0.10307138,  0.17590734, -0.12607832,\n",
       "          0.16134007,  0.09061891,  0.08405854, -0.09983118,  0.22531839,\n",
       "          0.24946068], dtype=float32)>,\n",
       "  <tf.Variable 'dense_83_1/kernel:0' shape=(16, 3) dtype=float32, numpy=\n",
       "  array([[-0.15199758,  0.3762409 ,  0.40500244],\n",
       "         [-0.3052415 , -0.28328368,  0.25130093],\n",
       "         [ 0.6362479 , -0.30690926,  0.0119264 ],\n",
       "         [-0.22179146, -0.4248141 , -0.01072714],\n",
       "         [ 0.1356743 , -0.26963627,  0.10803621],\n",
       "         [-0.43506643,  0.5202995 , -0.157995  ],\n",
       "         [-0.04289003, -0.5346143 , -0.46062535],\n",
       "         [-0.4018652 ,  0.16653748,  0.56430465],\n",
       "         [ 0.37992966,  0.41991806, -0.29337436],\n",
       "         [ 0.38785365,  0.06055215,  0.65354884],\n",
       "         [ 0.43118557, -0.2150142 ,  0.0706815 ],\n",
       "         [ 0.5181592 , -0.43425444, -0.1871244 ],\n",
       "         [-0.6984766 ,  0.41324067,  0.25359246],\n",
       "         [-0.33117157, -0.19153072,  0.49148777],\n",
       "         [ 0.60333437, -0.02115818, -0.27425626],\n",
       "         [ 0.22546342,  0.4962075 , -0.5748808 ]], dtype=float32)>,\n",
       "  <tf.Variable 'dense_83_1/bias:0' shape=(3,) dtype=float32, numpy=array([ 0.13094173,  0.12483255, -0.17233789], dtype=float32)>]}"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(model5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting autokeras\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1c/f6/fede04262a0c7244cb36fe85449c72629d148f37aa3cad76b4dab2409cbe/autokeras-0.3.7.tar.gz (97kB)\n",
      "\u001b[K    100% |████████████████████████████████| 102kB 307kB/s a 0:00:01\n",
      "\u001b[?25hCollecting scipy==1.2.0 (from autokeras)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c2/7b/370891c7686e598c59890588722c1d1c2b485953c4b127b7b752accbddb6/scipy-1.2.0-cp37-cp37m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl (28.7MB)\n",
      "\u001b[K    100% |████████████████████████████████| 28.7MB 522kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting torch==1.0.1 (from autokeras)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b1/a6/414a603f24eee6df88da1d087fe358a13d32f8d571eba71520f4620d96be/torch-1.0.1-cp37-none-macosx_10_7_x86_64.whl (58.9MB)\n",
      "\u001b[K    100% |████████████████████████████████| 58.9MB 408kB/s ta 0:00:011    49% |███████████████▉                | 29.1MB 10.3MB/s eta 0:00:03    97% |███████████████████████████████▎| 57.6MB 4.7MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting torchvision==0.2.1 (from autokeras)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ca/0d/f00b2885711e08bd71242ebe7b96561e6f6d01fdb4b9dcf4d37e2e13c5e1/torchvision-0.2.1-py2.py3-none-any.whl (54kB)\n",
      "\u001b[K    100% |████████████████████████████████| 61kB 9.2MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting numpy==1.15.4 (from autokeras)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3d/c3/a69406093c9a780a74964f41cd56b06c0346d686a9b3f392d123a663f5e0/numpy-1.15.4-cp37-cp37m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl (24.5MB)\n",
      "\u001b[K    100% |████████████████████████████████| 24.5MB 1.2MB/s ta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: keras==2.2.4 in /Users/jung-kyoyoon/anaconda3/lib/python3.7/site-packages (from autokeras) (2.2.4)\n",
      "Collecting scikit-learn==0.20.2 (from autokeras)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/11/0f/e2279fee7f9834c63b24fe64515412fd21dd81e82adcf6c79dcc93bb8e6a/scikit_learn-0.20.2-cp37-cp37m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl (7.8MB)\n",
      "\u001b[K    100% |████████████████████████████████| 7.8MB 2.9MB/s ta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: scikit-image==0.14.2 in /Users/jung-kyoyoon/anaconda3/lib/python3.7/site-packages (from autokeras) (0.14.2)\n",
      "Collecting tqdm==4.29.1 (from autokeras)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ed/d6/3458d39cf4978f4ece846295e83daf5ece710ab0a4106774f7f7b3a68697/tqdm-4.29.1-py2.py3-none-any.whl (46kB)\n",
      "\u001b[K    100% |████████████████████████████████| 51kB 7.6MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorflow==1.12.0 (from autokeras)\n",
      "\u001b[31m  Could not find a version that satisfies the requirement tensorflow==1.12.0 (from autokeras) (from versions: 1.13.0rc1, 1.13.0rc2, 1.13.1, 2.0.0a0)\u001b[0m\n",
      "\u001b[31mNo matching distribution found for tensorflow==1.12.0 (from autokeras)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#!pip install autokeras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow==1.12.0\n",
      "\u001b[31m  Could not find a version that satisfies the requirement tensorflow==1.12.0 (from versions: 1.13.0rc1, 1.13.0rc2, 1.13.1, 2.0.0a0)\u001b[0m\n",
      "\u001b[31mNo matching distribution found for tensorflow==1.12.0\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#!pip install tensorflow==1.12.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
